{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_pf_zpA71Er"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "üöÄ Vintern Embedding API Server - B·∫¢N BYPASS (Final Fix)\n",
        "‚ö†Ô∏è Y√äU C·∫¶U: ƒê√£ Restart Runtime tr∆∞·ªõc khi ch·∫°y!\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from unittest.mock import MagicMock\n",
        "\n",
        "# ============================================================================\n",
        "# B∆Ø·ªöC 0: T·∫†O \"THU·ªêC GI·∫¢\" (MOCKING FLASH_ATTN) - QUAN TR·ªåNG NH·∫§T\n",
        "# ============================================================================\n",
        "print(\"üíä ƒêang ti√™m 'thu·ªëc gi·∫£' Flash Attention ƒë·ªÉ ƒë√°nh l·ª´a b·ªô ki·ªÉm tra...\")\n",
        "\n",
        "# 1. T·∫°o m·ªôt module gi·∫£\n",
        "mock_flash_attn = MagicMock()\n",
        "# Fix l·ªói \"__spec__ is not set\" b·∫±ng c√°ch g√°n spec gi·∫£\n",
        "mock_flash_attn.__spec__ = MagicMock()\n",
        "\n",
        "# 2. G√°n v√†o h·ªá th·ªëng (Python s·∫Ω nghƒ© l√† th∆∞ vi·ªán n√†y ƒë√£ ƒë∆∞·ª£c c√†i)\n",
        "sys.modules[\"flash_attn\"] = mock_flash_attn\n",
        "sys.modules[\"flash_attn.flash_attn_interface\"] = mock_flash_attn\n",
        "sys.modules[\"flash_attn.bert_padding\"] = mock_flash_attn\n",
        "\n",
        "print(\"‚úÖ ƒê√£ bypass th√†nh c√¥ng b∆∞·ªõc ki·ªÉm tra th∆∞ vi·ªán!\")\n",
        "\n",
        "# ============================================================================\n",
        "# B∆Ø·ªöC 1: D·ªåN D·∫∏P CACHE (ƒê·ªÇ TR√ÅNH XUNG ƒê·ªòT C≈®)\n",
        "# ============================================================================\n",
        "cache_dir = \"/root/.cache/huggingface/modules/transformers_modules/5CD-AI/Vintern-Embedding-1B\"\n",
        "if os.path.exists(cache_dir):\n",
        "    print(\"üóëÔ∏è ƒêang x√≥a cache model c≈©...\")\n",
        "    shutil.rmtree(cache_dir, ignore_errors=True)\n",
        "\n",
        "# ============================================================================\n",
        "# B∆Ø·ªöC 2: C√ÄI ƒê·∫∂T DEPENDENCIES (NHANH G·ªåN)\n",
        "# ============================================================================\n",
        "print(\"üì¶ ƒêang c√†i ƒë·∫∑t packages c·∫ßn thi·∫øt...\")\n",
        "import subprocess\n",
        "\n",
        "packages = [\n",
        "    \"transformers==4.48.0\",\n",
        "    \"torch\",\n",
        "    \"torchvision\",\n",
        "    \"Pillow\",\n",
        "    \"flask\",\n",
        "    \"flask-cors\",\n",
        "    \"pyngrok\",\n",
        "    \"timm\",\n",
        "    \"einops\",\n",
        "    \"decord\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "\n",
        "print(\"‚úÖ Packages ƒë√£ s·∫µn s√†ng!\")\n",
        "\n",
        "# ============================================================================\n",
        "# B∆Ø·ªöC 3: LOAD MODEL (CH·∫æ ƒê·ªò EAGER)\n",
        "# ============================================================================\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "model_name = \"5CD-AI/Vintern-Embedding-1B\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"üî• Device: {device.upper()}\")\n",
        "print(f\"üì• ƒêang load model {model_name}...\")\n",
        "print(f\"üõ°Ô∏è Ch·∫ø ƒë·ªô: Eager Attention (Bypass Flash Attn)\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# V√¨ ƒë√£ Mock ·ªü B∆∞·ªõc 0, transformers s·∫Ω cho qua b∆∞·ªõc check import\n",
        "# attn_implementation=\"eager\" s·∫Ω ƒë·∫£m b·∫£o model ch·∫°y logic th∆∞·ªùng, kh√¥ng g·ªçi v√†o c√°i Mock\n",
        "model = AutoModel.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"\n",
        ").eval()\n",
        "\n",
        "if device == \"cuda\":\n",
        "    model = model.cuda()\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded successfully! (Server ƒë√£ l√™n)\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# B∆Ø·ªöC 4: API SERVER\n",
        "# ============================================================================\n",
        "def base64_to_image(base64_str):\n",
        "    img_bytes = base64.b64decode(base64_str)\n",
        "    return Image.open(io.BytesIO(img_bytes))\n",
        "\n",
        "def tensor_to_base64(tensor):\n",
        "    buffer = io.BytesIO()\n",
        "    numpy_array = tensor.cpu().float().numpy()\n",
        "    np.save(buffer, numpy_array)\n",
        "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "\n",
        "def base64_to_tensor(base64_str):\n",
        "    tensor_bytes = base64.b64decode(base64_str)\n",
        "    buffer = io.BytesIO(tensor_bytes)\n",
        "    numpy_array = np.load(buffer, allow_pickle=False)\n",
        "    tensor = torch.from_numpy(numpy_array)\n",
        "    if device == \"cuda\":\n",
        "        tensor = tensor.cuda()\n",
        "    return tensor\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"ok\", \"device\": device})\n",
        "\n",
        "@app.route('/encode_images', methods=['POST'])\n",
        "def encode_images():\n",
        "    try:\n",
        "        data = request.json\n",
        "        images_b64 = data['images']\n",
        "        images = [base64_to_image(img_b64) for img_b64 in images_b64]\n",
        "        batch_images = processor.process_images(images)\n",
        "        if device == \"cuda\":\n",
        "            batch_images[\"pixel_values\"] = batch_images[\"pixel_values\"].cuda().bfloat16()\n",
        "            batch_images[\"input_ids\"] = batch_images[\"input_ids\"].cuda()\n",
        "            batch_images[\"attention_mask\"] = batch_images[\"attention_mask\"].cuda().bfloat16()\n",
        "        else:\n",
        "            batch_images[\"pixel_values\"] = batch_images[\"pixel_values\"].float()\n",
        "            batch_images[\"attention_mask\"] = batch_images[\"attention_mask\"].float()\n",
        "        with torch.no_grad():\n",
        "            embeddings = model(**batch_images)\n",
        "        return jsonify({\"embeddings\": [tensor_to_base64(embeddings[i]) for i in range(len(images))]})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/encode_texts', methods=['POST'])\n",
        "def encode_texts():\n",
        "    try:\n",
        "        data = request.json\n",
        "        texts = data['texts']\n",
        "        batch_texts = processor.process_docs(texts)\n",
        "        if device == \"cuda\":\n",
        "            batch_texts[\"input_ids\"] = batch_texts[\"input_ids\"].cuda()\n",
        "            batch_texts[\"attention_mask\"] = batch_texts[\"attention_mask\"].cuda().bfloat16()\n",
        "        else:\n",
        "            batch_texts[\"attention_mask\"] = batch_texts[\"attention_mask\"].float()\n",
        "        with torch.no_grad():\n",
        "            embeddings = model(**batch_texts)\n",
        "        return jsonify({\"embeddings\": [tensor_to_base64(embeddings[i]) for i in range(len(texts))]})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/encode_query', methods=['POST'])\n",
        "def encode_query():\n",
        "    try:\n",
        "        data = request.json\n",
        "        query = data['query']\n",
        "        batch_query = processor.process_queries([query])\n",
        "        if device == \"cuda\":\n",
        "            batch_query[\"input_ids\"] = batch_query[\"input_ids\"].cuda()\n",
        "            batch_query[\"attention_mask\"] = batch_query[\"attention_mask\"].cuda().bfloat16()\n",
        "        else:\n",
        "            batch_query[\"attention_mask\"] = batch_query[\"attention_mask\"].float()\n",
        "        with torch.no_grad():\n",
        "            embedding = model(**batch_query)\n",
        "        return jsonify({\"embedding\": tensor_to_base64(embedding)})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/compute_similarity', methods=['POST'])\n",
        "def compute_similarity():\n",
        "    try:\n",
        "        data = request.json\n",
        "        query_emb_b64 = data['query_embedding']\n",
        "        docs_emb_b64 = data['doc_embeddings']\n",
        "        query_embedding = base64_to_tensor(query_emb_b64)\n",
        "        doc_embeddings = [base64_to_tensor(emb_b64) for emb_b64 in docs_emb_b64]\n",
        "        scores = processor.score_multi_vector(query_embedding, doc_embeddings)\n",
        "        return jsonify({\"scores\": tensor_to_base64(scores[0])})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# ============================================================================\n",
        "# B∆Ø·ªöC 5: NGROK\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üåê ƒêang kh·ªüi ƒë·ªông Ngrok...\")\n",
        "NGROK_AUTH_TOKEN = \"thay_ƒë·ªïi_b·∫±ng_token_c·ªßa_b·∫°n_nh√©\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(5000)\n",
        "\n",
        "print(f\"üéâ SERVER ƒê√É CH·∫†Y TH√ÄNH C√îNG!\")\n",
        "print(f\"üåê Public URL: {public_url}\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}