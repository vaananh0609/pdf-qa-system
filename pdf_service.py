import os
import shutil
import time
from datetime import datetime
from typing import List, Dict
from pdf_processor import PDFProcessor
from database import db
from gemini_service import GeminiService
from vintern_client import get_vintern_client
from config import Config
import logging
import hashlib

class PDFService:
    def __init__(self):
        self.processor = PDFProcessor()
        self.gemini = GeminiService()
        self.upload_folder = 'uploads'
        
        # T·∫°o th∆∞ m·ª•c uploads n·∫øu ch∆∞a c√≥
        if not os.path.exists(self.upload_folder):
            os.makedirs(self.upload_folder)

    def _log_event(self, event_type: str, details: Dict):
        """L∆∞u log h·ªá th·ªëng v√†o collection system_logs."""
        try:
            log_entry = {
                'event_type': event_type,
                'timestamp': datetime.now(),
                'details': details
            }
            db.insert_document('system_logs', log_entry)
        except Exception as log_error:
            logging.warning(f"Kh√¥ng th·ªÉ ghi log {event_type}: {log_error}")
    
    def upload_pdf(self, file, filename: str, caption: str = '') -> Dict:
        """Upload v√† x·ª≠ l√Ω file PDF - h·ªó tr·ª£ c·∫£ PDF text v√† PDF scanned"""
        operation_start = time.time()
        try:
            print(f"B·∫Øt ƒë·∫ßu upload_pdf: {filename}")
            
            # L∆∞u file v√†o th∆∞ m·ª•c uploads
            file_path = os.path.join(self.upload_folder, filename)
            print(f"L∆∞u file v√†o: {file_path}")
            file.save(file_path)
            
            # Ph√¢n t√≠ch t·ª´ng trang ƒë·ªÉ h·ªó tr·ª£ file mixed (text + scanned)
            page_infos = self.processor.analyze_pdf_pages(file_path)
            total_pages = len(page_infos)
            text_pages = sum(1 for p in page_infos if p.get('is_text'))
            image_pages = total_pages - text_pages

            if total_pages == 0:
                return {'success': False, 'message': 'Kh√¥ng th·ªÉ ƒë·ªçc file PDF ho·∫∑c file r·ªóng'}

            if text_pages == 0:
                print(f"üì∑ PDF Scanned detected (to√†n b·ªô) - S·ª≠ d·ª•ng Vintern")
                pdf_mode = 'scanned'
                result = self._upload_scanned_pdf(file_path, filename, caption)
            elif image_pages == 0:
                print(f"üìù PDF Text detected (to√†n b·ªô) - S·ª≠ d·ª•ng semantic chunking")
                pdf_mode = 'text'
                result = self._upload_text_pdf(file_path, filename, caption)
            else:
                print(f"üîÄ PDF Mixed detected - x·ª≠ l√Ω t·ª´ng trang (text: {text_pages}, image: {image_pages})")
                pdf_mode = 'mixed'
                result = self._upload_mixed_pdf(file_path, filename, caption, page_infos)

            duration_ms = int((time.time() - operation_start) * 1000)
            self._log_event('upload', {
                'filename': filename,
                'caption': caption,
                'pdf_mode_detected': pdf_mode,
                'total_pages': total_pages,
                'text_pages': text_pages,
                'image_pages': image_pages,
                'duration_ms': duration_ms,
                'success': result.get('success'),
                'message': result.get('message'),
                'metadata_id': result.get('metadata_id')
            })
            return result
            
        except Exception as e:
            # X√≥a file n·∫øu c√≥ l·ªói
            file_path = os.path.join(self.upload_folder, filename)
            if os.path.exists(file_path):
                os.remove(file_path)
            logging.error(f"L·ªói upload PDF: {e}")
            duration_ms = int((time.time() - operation_start) * 1000)
            self._log_event('upload', {
                'filename': filename,
                'caption': caption,
                'pdf_mode_detected': 'unknown',
                'duration_ms': duration_ms,
                'success': False,
                'error': str(e)
            })
            return {'success': False, 'message': f'L·ªói upload: {str(e)}'}
    
    def _upload_text_pdf(self, file_path: str, filename: str, caption: str) -> Dict:
        """X·ª≠ l√Ω PDF text th∆∞·ªùng - gi·ªØ nguy√™n logic c≈©"""
        try:
            # X·ª≠ l√Ω PDF
            print(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω PDF text: {filename}")
            result = self.processor.process_pdf_file(file_path, caption)
            
            if not result:
                print(f"L·ªói x·ª≠ l√Ω PDF: {filename}")
                if os.path.exists(file_path):
                    os.remove(file_path)
                return {'success': False, 'message': 'Kh√¥ng th·ªÉ x·ª≠ l√Ω file PDF'}
            
            metadata = result['metadata']
            metadata['pdf_type'] = 'text'  # ƒê√°nh d·∫•u lo·∫°i PDF
            chunks = result['chunks']
            
            # L∆∞u metadata v√†o database
            metadata_id = db.insert_document('pdf_files', metadata)
            
            # L∆∞u chunks v√†o database
            chunk_ids = []
            for chunk in chunks:
                chunk['metadata_id'] = metadata_id
                chunk['chunk_type'] = 'text'  # ƒê√°nh d·∫•u lo·∫°i chunk
                chunk_id = db.insert_document('pdf_chunks', chunk)
                chunk_ids.append(chunk_id)
            
            return {
                'success': True,
                'message': 'Upload PDF text th√†nh c√¥ng',
                'metadata_id': str(metadata_id),
                'chunk_ids': [str(chunk_id) for chunk_id in chunk_ids],
                'total_chunks': len(chunks),
                'pdf_type': 'text'
            }
            
        except Exception as e:
            if os.path.exists(file_path):
                os.remove(file_path)
            logging.error(f"L·ªói x·ª≠ l√Ω PDF text: {e}")
            return {'success': False, 'message': f'L·ªói upload PDF text: {str(e)}'}
    
    def _upload_scanned_pdf(self, file_path: str, filename: str, caption: str) -> Dict:
        """X·ª≠ l√Ω PDF scanned - chuy·ªÉn th√†nh ·∫£nh v√† d√πng Vintern embedding"""
        try:
            print(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω PDF scanned: {filename}")
            
            # Chuy·ªÉn ƒë·ªïi PDF pages th√†nh images
            images = self.processor.convert_pdf_pages_to_images(file_path, max_pages=200)
            
            if not images:
                if os.path.exists(file_path):
                    os.remove(file_path)
                return {'success': False, 'message': 'Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi PDF th√†nh ·∫£nh'}
            
            # Kh·ªüi t·∫°o Vintern client (g·ªçi API Colab)
            vintern = get_vintern_client()
            
            # Set API URL n·∫øu c√≥ trong config
            if Config.VINTERN_API_URL:
                vintern.set_api_url(Config.VINTERN_API_URL)
            
            if not vintern.is_available():
                logging.warning("Vintern API kh√¥ng kh·∫£ d·ª•ng, fallback v·ªÅ x·ª≠ l√Ω text")
                return self._upload_text_pdf(file_path, filename, caption)
            
            # T·∫°o embeddings cho c√°c ·∫£nh (batch processing)
            print(f"ƒêang t·∫°o embeddings cho {len(images)} trang...")
            
            # CPU: gi·∫£m batch size ƒë·ªÉ tr√°nh out of memory
            # GPU: c√≥ th·ªÉ x·ª≠ l√Ω batch l·ªõn h∆°n
            import torch
            batch_size = 2 if not torch.cuda.is_available() else 8
            
            if not torch.cuda.is_available():
                print(f"‚ö†Ô∏è ƒêang ch·∫°y tr√™n CPU - Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t 5-10 ph√∫t cho {len(images)} trang")
            
            all_embeddings = []
            
            for i in range(0, len(images), batch_size):
                batch_images = images[i:i + batch_size]
                progress = f"[{i+len(batch_images)}/{len(images)}]"
                print(f"  {progress} ƒêang x·ª≠ l√Ω batch...")
                
                batch_embeddings = vintern.encode_images(batch_images)
                
                # Chuy·ªÉn t·ª´ng embedding th√†nh list
                for j in range(len(batch_images)):
                    all_embeddings.append(batch_embeddings[j])
            
            # T·∫°o metadata
            metadata = {
                'filename': filename,
                'file_path': file_path,
                'file_size': os.path.getsize(file_path),
                'caption': caption,
                'total_chunks': len(images),
                'total_pages': len(images),
                'pdf_type': 'scanned',  # ƒê√°nh d·∫•u lo·∫°i PDF
                'created_at': datetime.now(),
                'processed': True,
                'chunking_strategy': 'image-embedding',
                'file_id': hashlib.md5(filename.encode()).hexdigest()
            }
            
            # L∆∞u metadata v√†o database
            metadata_id = db.insert_document('pdf_files', metadata)
            
            # L∆∞u t·ª´ng trang (·∫£nh + embedding) v√†o database
            chunk_ids = []
            for page_num, (image, embedding) in enumerate(zip(images, all_embeddings)):
                # Chuy·ªÉn image th√†nh bytes
                image_bytes = self.processor.image_to_bytes(image, format='JPEG')
                
                # Chuy·ªÉn embedding th√†nh bytes
                embedding_bytes = vintern.embedding_to_bytes(embedding)
                
                chunk_data = {
                    'metadata_id': metadata_id,
                    'filename': filename,
                    'chunk_index': page_num,
                    'page_number': page_num + 1,
                    'chunk_type': 'image',  # ƒê√°nh d·∫•u lo·∫°i chunk
                    'image_data': image_bytes,  # L∆∞u ·∫£nh
                    'embedding_data': embedding_bytes,  # L∆∞u embedding
                    'created_at': datetime.now(),
                    'chunk_id': hashlib.md5(f"{filename}_{page_num}".encode()).hexdigest()
                }
                
                chunk_id = db.insert_document('pdf_chunks', chunk_data)
                chunk_ids.append(chunk_id)
            
            print(f"‚úÖ ƒê√£ l∆∞u {len(chunk_ids)} trang v√†o database")
            
            return {
                'success': True,
                'message': 'Upload PDF scanned th√†nh c√¥ng',
                'metadata_id': str(metadata_id),
                'chunk_ids': [str(chunk_id) for chunk_id in chunk_ids],
                'total_chunks': len(chunk_ids),
                'pdf_type': 'scanned'
            }
            
        except Exception as e:
            if os.path.exists(file_path):
                os.remove(file_path)
            logging.error(f"L·ªói x·ª≠ l√Ω PDF scanned: {e}")
            return {'success': False, 'message': f'L·ªói upload PDF scanned: {str(e)}'}
    
    def _upload_mixed_pdf(self, file_path: str, filename: str, caption: str, page_infos: List[Dict]) -> Dict:
        """X·ª≠ l√Ω PDF mixed: trang text s·∫Ω d√πng semantic chunking, trang image s·∫Ω d√πng Vintern embedding"""
        try:
            print(f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω PDF mixed: {filename}")

            # Kh·ªüi t·∫°o Vintern client n·∫øu c√≥ trang image
            vintern = None
            if any(not p.get('is_text') for p in page_infos):
                vintern = get_vintern_client()
                if Config.VINTERN_API_URL:
                    vintern.set_api_url(Config.VINTERN_API_URL)
                if not vintern.is_available():
                    logging.warning("Vintern API kh√¥ng kh·∫£ d·ª•ng, c√°c trang ·∫£nh s·∫Ω b·ªã b·ªè qua ho·∫∑c fallback v·ªÅ OCR n·∫øu implement")

            # Prepare containers
            text_chunks_all = []
            image_pages = []

            # Process text pages: create chunks per page and record
            global_chunk_index = 0
            for p in page_infos:
                if p.get('is_text'):
                    page_num = p.get('page_num')
                    page_text = p.get('text', '') or ''
                    page_text = self.processor._repair_extraction_artifacts(page_text)
                    if not page_text:
                        continue
                    page_chunks = self.processor.create_chunks(page_text, filename)
                    # Normalize and annotate chunks
                    for c in page_chunks:
                        c['filename'] = filename
                        c['page_number'] = page_num + 1
                        c['chunk_index'] = global_chunk_index
                        c['chunk_type'] = 'text'
                        c['created_at'] = c.get('created_at') or datetime.now()
                        c['chunk_id'] = c.get('chunk_id') or hashlib.md5(f"{filename}_{global_chunk_index}".encode()).hexdigest()
                        text_chunks_all.append(c)
                        global_chunk_index += 1
                else:
                    image_pages.append(p.get('page_num'))

            # Process image pages: convert to images and create embeddings via Vintern
            image_chunks = []
            all_embeddings = []
            images_for_encoding = []
            image_page_order = []

            for page_num in image_pages:
                img = self.processor.convert_pdf_page_to_image(file_path, page_num)
                if img is not None:
                    images_for_encoding.append(img)
                    image_page_order.append(page_num)

            if images_for_encoding and vintern and vintern.is_available():
                import torch
                batch_size = 2 if not torch.cuda.is_available() else 8
                for i in range(0, len(images_for_encoding), batch_size):
                    batch = images_for_encoding[i:i+batch_size]
                    batch_embeddings = vintern.encode_images(batch)
                    for emb in batch_embeddings:
                        all_embeddings.append(emb)

            # Build image chunk records (embedding may be missing if vintern unavailable)
            for idx, page_num in enumerate(image_page_order):
                img = images_for_encoding[idx]
                embedding = all_embeddings[idx] if idx < len(all_embeddings) else None

                image_bytes = self.processor.image_to_bytes(img, format='JPEG') if img is not None else None
                embedding_bytes = vintern.embedding_to_bytes(embedding) if (embedding is not None and vintern) else None

                chunk_data = {
                    'filename': filename,
                    'chunk_index': global_chunk_index,
                    'page_number': page_num + 1,
                    'chunk_type': 'image',
                    'image_data': image_bytes,
                    'embedding_data': embedding_bytes,
                    'created_at': datetime.now(),
                    'chunk_id': hashlib.md5(f"{filename}_{global_chunk_index}".encode()).hexdigest()
                }

                image_chunks.append(chunk_data)
                global_chunk_index += 1

            # Combine all chunks (text first then image) and save
            total_chunks = len(text_chunks_all) + len(image_chunks)
            metadata = {
                'filename': filename,
                'file_path': file_path,
                'file_size': os.path.getsize(file_path),
                'caption': caption,
                'total_chunks': total_chunks,
                'total_pages': len(page_infos),
                'total_text_pages': sum(1 for p in page_infos if p.get('is_text')),
                'total_image_pages': sum(1 for p in page_infos if not p.get('is_text')),
                'pdf_type': 'mixed',
                'created_at': datetime.now(),
                'processed': True,
                'chunking_strategy': 'mixed',
                'file_id': hashlib.md5(filename.encode()).hexdigest()
            }

            metadata_id = db.insert_document('pdf_files', metadata)

            chunk_ids = []
            # Insert text chunks
            for c in text_chunks_all:
                c['metadata_id'] = metadata_id
                cid = db.insert_document('pdf_chunks', c)
                chunk_ids.append(cid)

            # Insert image chunks
            for c in image_chunks:
                c['metadata_id'] = metadata_id
                cid = db.insert_document('pdf_chunks', c)
                chunk_ids.append(cid)

            print(f"‚úÖ ƒê√£ l∆∞u mixed PDF: {len(chunk_ids)} chunks (text {len(text_chunks_all)}, image {len(image_chunks)})")

            return {
                'success': True,
                'message': 'Upload PDF mixed th√†nh c√¥ng',
                'metadata_id': str(metadata_id),
                'chunk_ids': [str(chunk_id) for chunk_id in chunk_ids],
                'total_chunks': len(chunk_ids),
                'pdf_type': 'mixed'
            }

        except Exception as e:
            if os.path.exists(file_path):
                os.remove(file_path)
            logging.error(f"L·ªói x·ª≠ l√Ω PDF mixed: {e}")
            return {'success': False, 'message': f'L·ªói upload PDF mixed: {str(e)}'}
    
    
    def get_all_pdfs(self) -> List[Dict]:
        """L·∫•y danh s√°ch t·∫•t c·∫£ PDF files (s·∫Øp x·∫øp theo th·ªùi gian m·ªõi nh·∫•t)"""
        try:
            pdfs = db.find_documents('pdf_files', {'processed': True}, sort=[('created_at', -1)])
            return pdfs
        except Exception as e:
            print(f"L·ªói l·∫•y danh s√°ch PDF: {e}")
            return []
    
    def delete_pdf(self, filename: str) -> Dict:
        """X√≥a PDF file v√† c√°c chunks li√™n quan"""
        try:
            # T√¨m metadata c·ªßa file
            pdf_file = db.find_documents('pdf_files', {'filename': filename})
            
            if not pdf_file:
                return {'success': False, 'message': 'File kh√¥ng t·ªìn t·∫°i'}
            
            metadata_id = pdf_file[0]['_id']
            
            # X√≥a t·∫•t c·∫£ chunks li√™n quan (delete_many)
            db.delete_documents('pdf_chunks', {'metadata_id': metadata_id})
            
            # X√≥a metadata
            db.delete_document('pdf_files', {'_id': metadata_id})
            
            # X√≥a file v·∫≠t l√Ω (∆∞u ti√™n ƒë∆∞·ªùng d·∫´n l∆∞u trong metadata)
            file_path = pdf_file[0].get('file_path') or os.path.join(self.upload_folder, filename)
            try:
                if file_path and os.path.exists(file_path):
                    os.remove(file_path)
            except Exception:
                # B·ªè qua l·ªói x√≥a file v·∫≠t l√Ω ƒë·ªÉ kh√¥ng ch·∫∑n vi·ªác x√≥a DB
                pass
            
            return {'success': True, 'message': 'ƒê√£ x√≥a file th√†nh c√¥ng'}
            
        except Exception as e:
            return {'success': False, 'message': f'L·ªói x√≥a file: {str(e)}'}
    
    def search_and_answer(self, question: str, filename: str = None) -> Dict:
        """T√¨m ki·∫øm v√† tr·∫£ l·ªùi c√¢u h·ªèi"""
        query_start = time.time()
        try:
            # T√¨m t·∫•t c·∫£ chunks (ho·∫∑c chunks c·ªßa file c·ª• th·ªÉ)
            query = {}
            if filename:
                query['filename'] = filename
            
            all_chunks = db.find_documents('pdf_chunks', query)
            
            if not all_chunks:
                duration_ms = int((time.time() - query_start) * 1000)
                self._log_event('query', {
                    'question': question,
                    'filename_filter': filename,
                    'total_chunks_scanned': 0,
                    'relevant_chunks': 0,
                    'duration_ms': duration_ms,
                    'success': False,
                    'reason': 'no_chunks'
                })
                return {
                    'success': False,
                    'message': 'Kh√¥ng t√¨m th·∫•y t√†i li·ªáu ƒë·ªÉ t√¨m ki·∫øm'
                }
            
            # T√¨m chunks li√™n quan
            relevant_chunks = self.gemini.find_relevant_chunks(question, all_chunks)
            
            if not relevant_chunks:
                duration_ms = int((time.time() - query_start) * 1000)
                self._log_event('query', {
                    'question': question,
                    'filename_filter': filename,
                    'total_chunks_scanned': len(all_chunks),
                    'relevant_chunks': 0,
                    'duration_ms': duration_ms,
                    'success': False,
                    'reason': 'no_relevant_chunks'
                })
                return {
                    'success': False,
                    'message': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi'
                }
            
            # T·∫°o c√¢u tr·∫£ l·ªùi
            answer_result = self.gemini.generate_answer(question, relevant_chunks)
            duration_ms = int((time.time() - query_start) * 1000)
            filenames_used = list({chunk.get('filename') for chunk in relevant_chunks if chunk.get('filename')})
            self._log_event('query', {
                'question': question,
                'filename_filter': filename,
                'total_chunks_scanned': len(all_chunks),
                'relevant_chunks': len(relevant_chunks),
                'duration_ms': duration_ms,
                'success': True,
                'filenames_used': filenames_used
            })
            return {
                'success': True,
                'question': question,
                'answer': answer_result['answer'],
                'sources': answer_result['sources'],
                'relevant_chunks': len(relevant_chunks),
                'total_chunks_searched': len(all_chunks)
            }
            
        except Exception as e:
            duration_ms = int((time.time() - query_start) * 1000)
            self._log_event('query', {
                'question': question,
                'filename_filter': filename,
                'duration_ms': duration_ms,
                'success': False,
                'error': str(e)
            })
            return {
                'success': False,
                'message': f'L·ªói t√¨m ki·∫øm: {str(e)}'
            }
    
    def get_pdf_content(self, filename: str) -> Dict:
        """L·∫•y n·ªôi dung PDF ƒë·ªÉ hi·ªÉn th·ªã"""
        try:
            # T√¨m metadata
            pdf_file = db.find_documents('pdf_files', {'filename': filename})
            
            if not pdf_file:
                return {'success': False, 'message': 'File kh√¥ng t·ªìn t·∫°i'}
            
            # L·∫•y t·∫•t c·∫£ chunks c·ªßa file (KH√îNG l·∫•y binary data)
            chunks_raw = db.find_documents('pdf_chunks', {'filename': filename})
            
            # X·ª≠ l√Ω chunks: lo·∫°i b·ªè binary data cho image chunks
            chunks = []
            for chunk in chunks_raw:
                chunk_type = chunk.get('chunk_type', 'text')
                
                if chunk_type == 'image':
                    # Image chunk: kh√¥ng tr·∫£ v·ªÅ image_data v√† embedding_data nh∆∞ng c√≥ URL
                    chunks.append({
                        '_id': chunk.get('_id'),
                        'filename': chunk.get('filename'),
                        'chunk_index': chunk.get('chunk_index'),
                        'page_number': chunk.get('page_number'),
                        'chunk_type': 'image',
                        'text': f'[Trang {chunk.get("page_number")} - ·∫¢nh t·ª´ PDF scanned]',
                        'image_url': f'/pdf_image/{chunk.get("chunk_id")}',  # URL ƒë·ªÉ l·∫•y ·∫£nh
                        'created_at': chunk.get('created_at'),
                        'chunk_id': chunk.get('chunk_id')
                    })
                else:
                    # Text chunk: gi·ªØ nguy√™n
                    chunks.append(chunk)
            
            # S·∫Øp x·∫øp theo chunk_index
            chunks.sort(key=lambda x: x['chunk_index'])
            
            return {
                'success': True,
                'metadata': pdf_file[0],
                'chunks': chunks
            }
            
        except Exception as e:
            logging.error(f"L·ªói get_pdf_content: {e}")
            return {'success': False, 'message': f'L·ªói l·∫•y n·ªôi dung: {str(e)}'}
