# -*- coding: utf-8 -*-
"""
üöÄ Vintern Embedding API Server - Ch·∫°y tr√™n Colab GPU

Copy to√†n b·ªô file n√†y v√†o Google Colab v√† ch·∫°y!

H∆∞·ªõng d·∫´n:
1. M·ªü Google Colab: https://colab.research.google.com/
2. Runtime ‚Üí Change runtime type ‚Üí GPU
3. Copy to√†n b·ªô code n√†y v√†o 1 cell
4. Ch·∫°y cell
5. Copy URL t·ª´ output v√† d√°n v√†o config.py (LOCAL)
"""

# ============================================================================
# B∆Ø·ªöC 0: X√ìA CACHE C≈® (N·∫æU C√ì)
# ============================================================================
import shutil
import os

cache_dir = "/root/.cache/huggingface/modules/transformers_modules/5CD-AI/Vintern-Embedding-1B"
if os.path.exists(cache_dir):
    print(f"üóëÔ∏è X√≥a cache c≈©...")
    shutil.rmtree(cache_dir, ignore_errors=True)
    print("‚úÖ ƒê√£ x√≥a cache")

# ============================================================================
# B∆Ø·ªöC 1: C√ÄI ƒê·∫∂T DEPENDENCIES
# ============================================================================
print("üì¶ ƒêang c√†i ƒë·∫∑t packages...")
import subprocess
import sys

packages = [
    "transformers==4.48.0",
    "torch",
    "torchvision",
    "Pillow",
    "flask",
    "flask-cors",
    "pyngrok",
    "timm",
    "einops",
    "decord",
    "ninja",  # Required for flash_attn
    "packaging"  # Required for flash_attn
]

# C√†i flash_attn ri√™ng (c·∫ßn compile)
print("‚ö° ƒêang c√†i flash-attn (m·∫•t ~2-3 ph√∫t)...")
try:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "flash-attn", "--no-build-isolation"])
    print("‚úÖ flash-attn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t!")
except Exception as e:
    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ c√†i flash-attn: {e}")
    print("Model v·∫´n c√≥ th·ªÉ ch·∫°y nh∆∞ng ch·∫≠m h∆°n m·ªôt ch√∫t")

for package in packages:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])

print("‚úÖ Packages ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t!")

# ============================================================================
# B∆Ø·ªöC 2: IMPORT LIBRARIES
# ============================================================================
print("üìö ƒêang import libraries...")

import torch
from PIL import Image
from transformers import AutoModel, AutoProcessor
import numpy as np
import base64
import io
from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok
import threading

print("‚úÖ Import th√†nh c√¥ng!")

# ============================================================================
# B∆Ø·ªöC 3: LOAD VINTERN MODEL
# ============================================================================
model_name = "5CD-AI/Vintern-Embedding-1B"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"\n{'='*80}")
print(f"üî• Device: {device.upper()}")
print(f"üì• ƒêang load model {model_name}...")
print(f"‚è≥ Qu√° tr√¨nh n√†y m·∫•t ~2-5 ph√∫t l·∫ßn ƒë·∫ßu...")
print(f"{'='*80}\n")

processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)

model = AutoModel.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
    low_cpu_mem_usage=True,
    trust_remote_code=True,
).eval()

if device == "cuda":
    model = model.cuda()

print(f"\n‚úÖ Model loaded successfully!\n")

# ============================================================================
# B∆Ø·ªöC 4: HELPER FUNCTIONS
# ============================================================================
def base64_to_image(base64_str):
    """Convert base64 to PIL Image"""
    img_bytes = base64.b64decode(base64_str)
    return Image.open(io.BytesIO(img_bytes))

def tensor_to_base64(tensor):
    """Convert tensor to base64"""
    buffer = io.BytesIO()
    # Convert to float32 ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi CPU
    numpy_array = tensor.cpu().float().numpy()
    np.save(buffer, numpy_array)
    return base64.b64encode(buffer.getvalue()).decode('utf-8')

def base64_to_tensor(base64_str):
    """Convert base64 to tensor"""
    tensor_bytes = base64.b64decode(base64_str)
    buffer = io.BytesIO(tensor_bytes)
    numpy_array = np.load(buffer, allow_pickle=False)
    tensor = torch.from_numpy(numpy_array)
    if device == "cuda":
        tensor = tensor.cuda()
    return tensor

# ============================================================================
# B∆Ø·ªöC 5: SETUP FLASK API
# ============================================================================
app = Flask(__name__)
CORS(app)

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "ok", "device": device})

@app.route('/encode_images', methods=['POST'])
def encode_images():
    try:
        data = request.json
        images_b64 = data['images']

        print(f"üì∏ Encoding {len(images_b64)} images...")

        # Convert base64 to PIL Images
        images = [base64_to_image(img_b64) for img_b64 in images_b64]

        # Process images
        batch_images = processor.process_images(images)

        # Move to device
        if device == "cuda":
            batch_images["pixel_values"] = batch_images["pixel_values"].cuda().bfloat16()
            batch_images["input_ids"] = batch_images["input_ids"].cuda()
            batch_images["attention_mask"] = batch_images["attention_mask"].cuda().bfloat16()
        else:
            batch_images["pixel_values"] = batch_images["pixel_values"].float()
            batch_images["input_ids"] = batch_images["input_ids"]
            batch_images["attention_mask"] = batch_images["attention_mask"].float()

        # Generate embeddings
        with torch.no_grad():
            embeddings = model(**batch_images)

        # Convert to base64
        embeddings_b64 = [tensor_to_base64(embeddings[i]) for i in range(len(images))]

        print(f"‚úÖ Done encoding {len(images)} images")

        return jsonify({"embeddings": embeddings_b64})

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/encode_texts', methods=['POST'])
def encode_texts():
    try:
        data = request.json
        texts = data['texts']

        print(f"üìù Encoding {len(texts)} texts...")

        # Process texts
        batch_texts = processor.process_docs(texts)

        # Move to device
        if device == "cuda":
            batch_texts["input_ids"] = batch_texts["input_ids"].cuda()
            batch_texts["attention_mask"] = batch_texts["attention_mask"].cuda().bfloat16()
        else:
            batch_texts["input_ids"] = batch_texts["input_ids"]
            batch_texts["attention_mask"] = batch_texts["attention_mask"].float()

        # Generate embeddings
        with torch.no_grad():
            embeddings = model(**batch_texts)

        # Convert to base64
        embeddings_b64 = [tensor_to_base64(embeddings[i]) for i in range(len(texts))]

        print(f"‚úÖ Done encoding {len(texts)} texts")

        return jsonify({"embeddings": embeddings_b64})

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/encode_query', methods=['POST'])
def encode_query():
    try:
        data = request.json
        query = data['query']

        print(f"üîç Encoding query: {query[:50]}...")

        # Process query
        batch_query = processor.process_queries([query])

        # Move to device
        if device == "cuda":
            batch_query["input_ids"] = batch_query["input_ids"].cuda()
            batch_query["attention_mask"] = batch_query["attention_mask"].cuda().bfloat16()
        else:
            batch_query["input_ids"] = batch_query["input_ids"]
            batch_query["attention_mask"] = batch_query["attention_mask"].float()

        # Generate embedding
        with torch.no_grad():
            embedding = model(**batch_query)

        # Convert to base64
        embedding_b64 = tensor_to_base64(embedding)

        print(f"‚úÖ Done encoding query")

        return jsonify({"embedding": embedding_b64})

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/compute_similarity', methods=['POST'])
def compute_similarity():
    try:
        data = request.json
        query_emb_b64 = data['query_embedding']
        docs_emb_b64 = data['doc_embeddings']

        print(f"üî¢ Computing similarity for {len(docs_emb_b64)} documents...")

        # Convert to tensors
        query_embedding = base64_to_tensor(query_emb_b64)
        doc_embeddings = [base64_to_tensor(emb_b64) for emb_b64 in docs_emb_b64]

        # Compute similarity
        scores = processor.score_multi_vector(query_embedding, doc_embeddings)

        # Convert to base64
        scores_b64 = tensor_to_base64(scores[0])

        print(f"‚úÖ Done computing similarity")

        return jsonify({"scores": scores_b64})

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return jsonify({"error": str(e)}), 500

# ============================================================================
# B∆Ø·ªöC 6: SETUP NGROK V√Ä CH·∫†Y SERVER
# ============================================================================
print("\n" + "="*80)
print("üåê ƒêang setup ngrok tunnel...")
print("="*80 + "\n")

# Set ngrok auth token
import os
NGROK_AUTH_TOKEN = os.environ.get("NGROK_AUTH_TOKEN")
if not NGROK_AUTH_TOKEN:
    raise RuntimeError("NGROK_AUTH_TOKEN ch∆∞a ƒë∆∞·ª£c set trong environment")
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Terminate any existing tunnels
ngrok.kill()

# Start ngrok tunnel
public_url = ngrok.connect(5000)

print("\n" + "="*80)
print("üéâ VINTERN API SERVER ƒêANG CH·∫†Y!")
print("="*80)
print(f"\nüåê Public URL: {public_url}")
print(f"\n‚ö†Ô∏è QUAN TR·ªåNG:")
print(f"   1. Copy URL b√™n tr√™n")
print(f"   2. M·ªü file config.py tr√™n LOCAL")
print(f"   3. Set: VINTERN_API_URL = '{public_url}'")
print(f"   4. Restart local app: python app.py")
print(f"\nüî• Server s·∫Ω ch·∫°y cho ƒë·∫øn khi b·∫°n stop cell n√†y...")
print(f"üí° Gi·ªØ tab Colab m·ªü ƒë·ªÉ tr√°nh b·ªã disconnect!")
print("="*80 + "\n")

# Run Flask server
app.run(port=5000)