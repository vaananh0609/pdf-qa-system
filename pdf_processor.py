from __future__ import annotations

import PyPDF2
import os
import hashlib
from datetime import datetime
import re
from typing import List, Dict
import google.generativeai as genai
from config import Config
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import logging
try:
    import underthesea
except Exception:
    underthesea = None
import json
try:
    import fitz  # PyMuPDF ƒë·ªÉ chuy·ªÉn PDF th√†nh ·∫£nh
except ImportError:
    fitz = None
try:
    from PIL import Image
except ImportError:
    Image = None

class PDFProcessor:
    def __init__(self):
        self.chunk_size = 1000  # S·ªë k√Ω t·ª± m·ªói chunk (fallback)
        self.chunk_overlap = 200  # S·ªë k√Ω t·ª± overlap gi·ªØa c√°c chunk
        # Kh·ªüi t·∫°o Gemini cho semantic chunking (n·∫øu c√≥ key t·ª´ environment)
        try:
            primary_key = None
            keys = getattr(Config, "GEMINI_API_KEYS", None)
            if isinstance(keys, list) and keys:
                primary_key = keys[0]
            else:
                primary_key = getattr(Config, "GEMINI_API_KEY", None)

            if primary_key:
                genai.configure(api_key=primary_key)
                self.model = genai.GenerativeModel('gemini-2.5-pro')
            else:
                self.model = None
        except Exception as e:
            logging.warning(f"L·ªói kh·ªüi t·∫°o Gemini cho PDFProcessor: {e}")
            self.model = None
        # Delay embedding model initialization to avoid slow startup
        self.embedding_model = None
        # Vietnamese sentence tokenizer
        self.use_underthesea = underthesea is not None
        # Whether to use AI to return offsets (fallback khi embedding l·ªói)
        # self.use_ai_offsets = True  # Commented out, no AI fallback
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Tr√≠ch xu·∫•t text t·ª´ file PDF"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"
                
                return text.strip()
        except Exception as e:
            logging.error(f"L·ªói ƒë·ªçc PDF: {e}")
            return ""

    def _repair_extraction_artifacts(self, text: str) -> str:
        """S·ª≠a artifact t·ª´ tr√¨nh tr√≠ch xu·∫•t PDF"""
        if not text:
            return text

        # 1) Remove spaces between digits
        text = re.sub(r'(?<=\d) (?=\d)', '', text)

        # 2) Remove spaces after dot when between digits
        text = re.sub(r'(?<=\d)\. (?=\d)', '.', text)

        # 3) Join single-letter token with following token when safe: pattern (word1) (single_letter) (word2)
        # We only join single_letter with word2 (keep space before single_letter) to fix cases like 'H√† N ·ªôi' -> 'H√† N·ªôi'
        def _join_single_letter(match):
            g1 = match.group(1)
            single = match.group(2)
            g3 = match.group(3)
            return f"{g1} {single}{g3}"

        text = re.sub(r"(\b\w+)\s+(\w)\s+(\w{2,})", _join_single_letter, text)

        return text
    
    def create_semantic_chunks(self, text: str, filename: str) -> List[Dict]:
        """T·∫°o semantic chunks t·ª´ text s·ª≠ d·ª•ng embedding ƒë·ªÉ gi·ªØ nguy√™n n·ªôi dung g·ªëc.

        √ù t∆∞·ªüng:
        - Kh√¥ng thay ƒë·ªïi (clean) vƒÉn b·∫£n g·ªëc khi t·∫°o chunk; ch·ªâ d√πng vƒÉn b·∫£n g·ªëc ƒë·ªÉ t·∫°o c√°c substring
        - T√°ch vƒÉn b·∫£n th√†nh c√¢u k√®m span (start/end)
        - T√≠nh embedding cho m·ªói c√¢u v√† gom c√°c c√¢u li√™n ti·∫øp c√≥ √Ω nghƒ©a th√†nh chunk d·ª±a tr√™n similarity
        - M·ªói chunk l√† substring nguy√™n v·∫πn c·ªßa vƒÉn b·∫£n g·ªëc (char_start/char_end ch√≠nh x√°c)
        """
        chunks: List[Dict] = []

        original_text = text or ""
        if not original_text:
            return chunks

        # N·∫øu qu√° ng·∫Øn (< 500 k√Ω t·ª±), t·∫°o 1 chunk duy nh·∫•t
        if len(original_text) < 500:
            chunk_data = {
                'filename': filename,
                'chunk_index': 0,
                'text': original_text,
                'page_number': 1,
                'char_start': 0,
                'char_end': len(original_text),
                'created_at': datetime.now(),
                'chunk_id': hashlib.md5(f"{filename}_0".encode()).hexdigest()
            }
            chunks.append(chunk_data)
            logging.info(f"‚úÖ VƒÉn b·∫£n qu√° ng·∫Øn, t·∫°o 1 chunk duy nh·∫•t cho file {filename}")
            return chunks

        estimated_pages = max(1, len(original_text) // 2000)

        # Th·ª≠ size-based chunking tr∆∞·ªõc (ch√≠nh)
        try:
            logging.info(f"üîç B·∫Øt ƒë·∫ßu chunking b·∫±ng k√≠ch th∆∞·ªõc cho file {filename}")
            # T√°ch th√†nh c√¢u k√®m v·ªã tr√≠ (span)
            sentences = self._split_into_sentences_with_spans(original_text)
            if not sentences:
                raise Exception("Kh√¥ng th·ªÉ t√°ch th√†nh c√¢u")

            logging.info(f"üìä T√°ch ƒë∆∞·ª£c {len(sentences)} c√¢u cho file {filename}")

            # Ensure embedding model is loaded (lazy) - c·∫ßn cho vi·ªác embed chunks
            if self.embedding_model is None:
                try:
                    self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                    logging.info("‚úÖ Embedding model loaded th√†nh c√¥ng")
                except Exception as e:
                    logging.warning(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o embedding model: {e}")
                    raise Exception("Embedding model kh√¥ng kh·∫£ d·ª•ng")

            # Gom c√°c c√¢u li√™n ti·∫øp th√†nh chunk d·ª±a tr√™n k√≠ch th∆∞·ªõc k√Ω t·ª±
            min_chars = 500
            max_chars = 1000

            chunk_index = 0
            i = 0
            n = len(sentences)

            while i < n:
                # Start a new chunk at sentence i
                start_span = sentences[i]['start']
                end_span = sentences[i]['end']
                curr_chars = len(sentences[i]['text'])

                j = i + 1
                while j < n:
                    next_len = len(sentences[j]['text'])
                    if curr_chars + next_len > max_chars:
                        if curr_chars >= min_chars:
                            break  # D·ª´ng, t·∫°o chunk hi·ªán t·∫°i
                        else:
                            # V·∫´n th√™m ƒë·ªÉ ƒë·∫°t min_chars, d√π v∆∞·ª£t max_chars m·ªôt ch√∫t
                            logging.info(f"  ‚ûï Th√™m c√¢u {j} ƒë·ªÉ ƒë·∫°t min_chars, chars={curr_chars}+{next_len}={curr_chars+next_len} (> {max_chars})")
                            end_span = sentences[j]['end']
                            curr_chars += next_len
                            j += 1
                    else:
                        # Th√™m c√¢u b√¨nh th∆∞·ªùng
                        logging.info(f"  ‚ûï Th√™m c√¢u {j}, chars={curr_chars}+{next_len}={curr_chars+next_len}")
                        end_span = sentences[j]['end']
                        curr_chars += next_len
                        j += 1

                # ƒêi·ªÅu ch·ªânh span ƒë·ªÉ kh√¥ng c·∫Øt token
                if 'prev_end' not in locals():
                    prev_end = 0
                adj_start, adj_end = self._adjust_span_to_token_boundary(original_text, start_span, end_span)
                if adj_start < prev_end:
                    adj_start = prev_end
                if adj_end <= adj_start:
                    adj_start, adj_end = start_span, end_span

                chunk_text = original_text[adj_start:adj_end]

                # Embed chunk text ƒë·ªÉ c√≥ vector cho retrieval
                try:
                    chunk_embedding = self.embedding_model.encode([chunk_text], convert_to_numpy=True)[0]
                    chunk_embedding_list = chunk_embedding.tolist()
                    logging.info(f"  üì• Embedded chunk {chunk_index}: {len(chunk_embedding_list)} dims")
                except Exception as e:
                    logging.warning(f"L·ªói embed chunk {chunk_index}: {e}, b·ªè qua embedding")
                    chunk_embedding_list = []

                chunk_data = {
                    'filename': filename,
                    'chunk_index': chunk_index,
                    'text': chunk_text,
                    'embedding': chunk_embedding_list,  # L∆∞u vector embedding
                    'page_number': min(chunk_index + 1, estimated_pages),
                    'char_start': adj_start,
                    'char_end': adj_end,
                    'created_at': datetime.now(),
                    'chunk_id': hashlib.md5(f"{filename}_{chunk_index}".encode()).hexdigest()
                }
                chunks.append(chunk_data)
                logging.info(f"  ‚úÖ T·∫°o chunk {chunk_index}: {len(chunk_text)} k√Ω t·ª± (c√¢u {i} ƒë·∫øn {j-1})")
                chunk_index += 1
                prev_end = adj_end
                i = j

            # Sanity check: ƒë·∫£m b·∫£o kh√¥ng m·∫•t k√Ω t·ª±
            total_chunk_chars = sum(len(c['text']) for c in chunks)
            if total_chunk_chars != len(original_text):
                logging.warning(f"T·ªïng k√Ω t·ª± chunks ({total_chunk_chars}) != k√Ω t·ª± g·ªëc ({len(original_text)})")

            logging.info(f"‚úÖ Chunking b·∫±ng k√≠ch th∆∞·ªõc th√†nh c√¥ng: {len(chunks)} chunks cho file {filename}")
            return chunks

        except Exception as e:
            logging.error(f"‚ùå L·ªói size-based chunking cho file {filename}: {e}")
            raise  # Kh√¥ng fallback, raise exception

        # # Fallback 1: AI-based offsets n·∫øu embedding l·ªói
        # if self.model and self.use_ai_offsets:
        #     try:
        #         logging.info(f"ü§ñ B·∫Øt ƒë·∫ßu chunking b·∫±ng AI fallback cho file {filename}")
        #         offsets = self._get_semantic_offsets_from_ai(original_text)
        #         if offsets and isinstance(offsets, list):
        #             # Validate and build chunks from offsets
        #             chunks = []
        #             prev_end = 0
        #             chunk_index = 0
        #             for off in offsets:
        #                 if not isinstance(off, dict):
        #                     continue
        #                 s = int(off.get('start', 0))
        #                 e = int(off.get('end', 0))
        #                 # clamp
        #                 s = max(0, min(s, len(original_text)))
        #                 e = max(0, min(e, len(original_text)))
        #                 if e <= s:
        #                     continue
        #                 # If gap exists before this offset, fill gap with a chunk (non-overlap)
        #                 if s > prev_end:
        #                     gap_text = original_text[prev_end:s]
        #                     chunks.append({
        #                         'filename': filename,
        #                         'chunk_index': chunk_index,
        #                         'text': gap_text,
        #                         'page_number': min(chunk_index + 1, estimated_pages),
        #                         'char_start': prev_end,
        #                         'char_end': s,
        #                         'created_at': datetime.now(),
        #                         'chunk_id': hashlib.md5(f"{filename}_{chunk_index}".encode()).hexdigest()
        #                     })
        #                     chunk_index += 1

        #                 chunk_text = original_text[s:e]
        #                 chunks.append({
        #                     'filename': filename,
        #                     'chunk_index': chunk_index,
        #                     'text': chunk_text,
        #                     'page_number': min(chunk_index + 1, estimated_pages),
        #                         'char_start': s,
        #                         'char_end': e,
        #                         'created_at': datetime.now(),
        #                         'chunk_id': hashlib.md5(f"{filename}_{chunk_index}".encode()).hexdigest()
        #                     })
        #                     chunk_index += 1
        #                     prev_end = e

        #                 # If there's trailing text after last offset, include it
        #                 if prev_end < len(original_text):
        #                     tail = original_text[prev_end:]
        #                     chunks.append({
        #                         'filename': filename,
        #                         'chunk_index': chunk_index,
        #                         'text': tail,
        #                         'page_number': min(chunk_index + 1, estimated_pages),
        #                         'char_start': prev_end,
        #                         'char_end': len(original_text),
        #                         'created_at': datetime.now(),
        #                         'chunk_id': hashlib.md5(f"{filename}_{chunk_index}".encode()).hexdigest()
        #                     })

        #                 # Validate total coverage (no loss). If mismatch, fallback to fixed-size
        #                 total_chunk_chars = sum(len(c['text']) for c in chunks)
        #                 if total_chunk_chars == len(original_text):
        #                     logging.info(f"‚úÖ Chunking b·∫±ng AI th√†nh c√¥ng: {len(chunks)} chunks cho file {filename}")
        #                     return chunks
        #                 else:
        #                     logging.warning(f"AI offsets coverage mismatch ({total_chunk_chars} != {len(original_text)}), falling back to fixed-size")
        #     except Exception as e:
        #         logging.warning(f"‚ùå L·ªói AI chunking cho file {filename}: {e}, d√πng fixed-size")

        # # Fallback cu·ªëi c√πng: fixed-size non-overlap
        # logging.info(f"üìè B·∫Øt ƒë·∫ßu chunking b·∫±ng fixed-size cho file {filename}")
        # chunks = self.create_fixed_size_chunks_nonoverlap(original_text, filename)
        # logging.info(f"‚úÖ Chunking b·∫±ng fixed-size th√†nh c√¥ng: {len(chunks)} chunks cho file {filename}")
        # return chunks

    def _split_into_sentences_with_spans(self, text: str) -> List[Dict]:
        """T√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u v·ªõi span (start/end)"""
        sentences: List[Dict] = []
        if not text:
            return sentences

        # If underthesea is available, use it for Vietnamese sentence tokenization
        if self.use_underthesea:
            try:
                v_sents = underthesea.sent_tokenize(text)
                # find spans by searching from last end to avoid mismatches
                cursor = 0
                for s in v_sents:
                    s_stripped = s.strip()
                    if not s_stripped:
                        continue
                    idx = text.find(s_stripped, cursor)
                    if idx == -1:
                        # fallback to regex method for remaining text
                        break
                    start = idx
                    end = idx + len(s_stripped)
                    sentences.append({'text': s_stripped, 'start': start, 'end': end})
                    cursor = end
                # If couldn't tokenize fully, fall back to regex for remaining parts
                if not sentences:
                    raise Exception('underthesea returned no sentences')
                return sentences
            except Exception:
                # fallback to regex below
                pass

        # Regex fallback: match up to a sentence end (., !, ?) or end of text. Use DOTALL to allow newlines.
        pattern = re.compile(r'.+?(?:[.!?]+|$)', re.S)
        for m in pattern.finditer(text):
            s = m.group(0)
            if s is None:
                continue
            if s.strip() == '':
                continue
            start, end = m.span()
            sentences.append({'text': s, 'start': start, 'end': end})

        return sentences

    def _adjust_span_to_token_boundary(self, text: str, start: int, end: int, max_extend: int = 50) -> tuple:
        """ƒêi·ªÅu ch·ªânh span ƒë·ªÉ kh√¥ng c·∫Øt gi·ªØa token"""
        n = len(text)

        def is_boundary_char(ch):
            if ch.isspace():
                return True
            if ch in '.,;:!?()[]{}"\'"-‚Äì‚Äî/\\':
                return True
            return False

        new_start = start
        new_end = end

        # Adjust start backward if it is in the middle of an alnum token
        if 0 < new_start < n and text[new_start].isalnum() and text[new_start-1].isalnum():
            # try move backward to previous boundary within max_extend
            limit = max(0, new_start - max_extend)
            found = False
            i = new_start
            while i > limit:
                if is_boundary_char(text[i-1]):
                    new_start = i
                    found = True
                    break
                i -= 1
            if not found:
                # fallback: move to the leftmost within limit
                new_start = max(limit, 0)

        # Adjust end forward if it is in the middle of an alnum token
        if 0 <= new_end < n and text[new_end-1].isalnum() and text[new_end].isalnum():
            # try move forward to next boundary within max_extend
            limit = min(n, new_end + max_extend)
            found = False
            i = new_end
            while i < limit:
                if is_boundary_char(text[i]):
                    new_end = i
                    found = True
                    break
                i += 1
            if not found:
                # fallback: move backward to previous boundary
                j = new_end
                while j > max(0, new_end - max_extend):
                    if is_boundary_char(text[j-1]):
                        new_end = j
                        found = True
                        break
                    j -= 1
                if not found:
                    new_end = new_end  # give up; leave as is

        # clamp
        new_start = max(0, min(new_start, n))
        new_end = max(new_start, min(new_end, n))

        return new_start, new_end

    def create_fixed_size_chunks_nonoverlap(self, text: str, filename: str) -> List[Dict]:
        """T·∫°o chunks kh√¥ng ch·ªìng (non-overlap) t·ª´ vƒÉn b·∫£n g·ªëc, kh√¥ng g·ªçi clean_text.

        D√πng khi kh√¥ng mu·ªën m·∫•t/ƒë·ªïi n·ªôi dung v√† c·∫ßn fallback an to√†n.
        """
        chunks = []
        original_text = text or ""
        if not original_text:
            return chunks

        estimated_pages = max(1, len(original_text) // 2000)
        start = 0
        chunk_index = 0

        while start < len(original_text):
            end = min(start + self.chunk_size, len(original_text))

            # Adjust only end to avoid splitting tokens for non-overlap fixed chunks
            _, new_end = self._adjust_span_to_token_boundary(original_text, start, end)
            if new_end <= start:
                new_end = end

            chunk_text = original_text[start:new_end]

            chunk_data = {
                'filename': filename,
                'chunk_index': chunk_index,
                'text': chunk_text,
                'page_number': min(chunk_index + 1, estimated_pages),
                'char_start': start,
                'char_end': new_end,
                'created_at': datetime.now(),
                'chunk_id': hashlib.md5(f"{filename}_{chunk_index}".encode()).hexdigest()
            }
            chunks.append(chunk_data)
            chunk_index += 1

            start = new_end

        return chunks

    def create_chunks(self, text: str, filename: str) -> List[Dict]:
        """API ƒë∆°n gi·∫£n ƒë·ªÉ t·∫°o chunks cho c√°c ƒëo·∫°n text nh·ªè (v√≠ d·ª• theo trang)."""
        if not text:
            return []
        cleaned = text.strip()
        if not cleaned:
            return []
        return self.create_fixed_size_chunks_nonoverlap(cleaned, filename)
    
    def _get_semantic_offsets_from_ai(self, text: str) -> List[Dict]:
        """Y√™u c·∫ßu AI tr·∫£ v·ªÅ offsets (start/end) cho semantic chunks"""
        if not self.model:
            return []

        try:
            # Build a strict prompt asking for JSON output only
            prompt = f"""
You are an expert text segmenter. I will provide a text. Do NOT change the text.
Return a JSON array of objects where each object has two integer fields: start and end.
Each start/end must be a character offset (0-based) in the exact text I provide. Offsets should partition the text into meaningful semantic chunks (each chunk a contiguous substring). Do NOT output any explanatory text, only the JSON array.

Text:
{text}

Requirements:
- Output must be valid JSON array like [{"start":0,"end":123},{"start":123,"end":456},...]
- Offsets must be in increasing order, non-overlapping. It's OK to omit very small fragments but prefer full coverage.
"""

            response = self.model.generate_content(prompt)
            resp_text = response.text.strip()

            # Try to load JSON directly
            try:
                data = json.loads(resp_text)
                if isinstance(data, list):
                    # sanitize items
                    out = []
                    for item in data:
                        if isinstance(item, dict) and 'start' in item and 'end' in item:
                            out.append({'start': int(item['start']), 'end': int(item['end'])})
                    return out
            except Exception:
                pass

            # Try to extract a JSON substring from the response
            jstart = resp_text.find('[')
            jend = resp_text.rfind(']')
            if jstart != -1 and jend != -1 and jend > jstart:
                sub = resp_text[jstart:jend+1]
                try:
                    data = json.loads(sub)
                    out = []
                    for item in data:
                        if isinstance(item, dict) and 'start' in item and 'end' in item:
                            out.append({'start': int(item['start']), 'end': int(item['end'])})
                    return out
                except Exception:
                    pass

            # Fallback: try to parse numbers in the response as pairs
            import re
            nums = re.findall(r'\d+', resp_text)
            if nums and len(nums) >= 2:
                pairs = []
                try:
                    it = iter(nums)
                    while True:
                        s = int(next(it))
                        e = int(next(it))
                        pairs.append({'start': s, 'end': e})
                except StopIteration:
                    pass
                return pairs

            return []

        except Exception as e:
            logging.warning(f"L·ªói khi g·ªçi AI tr·∫£ offsets: {e}")
            return []
    
    def process_pdf_file(self, pdf_path: str, caption: str = '') -> Dict:
        """X·ª≠ l√Ω file PDF v√† t·∫°o metadata"""
        try:
            filename = os.path.basename(pdf_path)
            
            # T√≠nh hash c·ªßa file (n·∫øu ch∆∞a c√≥ trong metadata)
            file_hash = ""
            try:
                hash_md5 = hashlib.md5()
                with open(pdf_path, "rb") as f:
                    for chunk in iter(lambda: f.read(4096), b""):
                        hash_md5.update(chunk)
                file_hash = hash_md5.hexdigest()
            except Exception as e:
                logging.warning(f"Kh√¥ng th·ªÉ t√≠nh hash file {pdf_path}: {e}")
            
            # Tr√≠ch xu·∫•t text
            text = self.extract_text_from_pdf(pdf_path)
            # Repair common extraction artifacts before chunking
            text = self._repair_extraction_artifacts(text)
            
            if not text:
                return None
            
            # T·∫°o chunks
            chunks = self.create_semantic_chunks(text, filename)
            
            # T·∫°o metadata
            total_chunk_chars = sum(len(c.get('text', '')) for c in chunks)
            char_mismatch = len(text) - total_chunk_chars

            metadata = {
                'filename': filename,
                'file_path': pdf_path,
                'file_size': os.path.getsize(pdf_path),
                'file_hash': file_hash,  # Th√™m file_hash v√†o metadata
                'caption': caption,
                'total_chunks': len(chunks),
                'total_text_length': len(text),
                'total_chunk_chars': total_chunk_chars,
                'char_mismatch': char_mismatch,
                'created_at': datetime.now(),
                'processed': True,
                'chunking_strategy': 'size-based-with-embedding',  # D·ª±a tr√™n k√≠ch th∆∞·ªõc, embed t·ª´ng chunk
                'file_id': hashlib.md5(filename.encode()).hexdigest()
            }
            
            return {
                'metadata': metadata,
                'chunks': chunks
            }
            
        except Exception as e:
            logging.error(f"L·ªói x·ª≠ l√Ω PDF {pdf_path}: {e}")
            return None
    
    def analyze_pdf_pages(self, pdf_path: str, threshold_chars: int = 50) -> List[Dict]:
        """
        Ph√¢n t√≠ch t·ª´ng trang: tr·∫£ v·ªÅ list c√°c dict {page_num, text, is_text}.
        
        Args:
            pdf_path: ƒê∆∞·ªùng d·∫´n file PDF
            threshold_chars: Ng∆∞·ª°ng s·ªë k√Ω t·ª± ƒë·ªÉ ph√¢n lo·∫°i trang l√† text hay image
            
        Returns:
            List[Dict] v·ªõi m·ªói dict c√≥:
                - page_num: s·ªë trang (0-based)
                - text: text tr√≠ch xu·∫•t ƒë∆∞·ª£c
                - is_text: True n·∫øu l√† trang text, False n·∫øu l√† trang image
        """
        page_infos = []
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                
                for page_num in range(total_pages):
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    text_length = len(text.strip())
                    
                    is_text = text_length >= threshold_chars
                    
                    page_infos.append({
                        'page_num': page_num,
                        'text': text,
                        'is_text': is_text,
                        'text_length': text_length
                    })
        except Exception as e:
            logging.error(f"L·ªói ph√¢n t√≠ch PDF pages: {e}")
        
        return page_infos
    
    def convert_pdf_page_to_image(self, pdf_path: str, page_num: int, zoom: float = 1.5):
        """Chuy·ªÉn 1 trang PDF th√†nh PIL Image (page_num l√† 0-based, t·ªëi ∆∞u t·ªëc ƒë·ªô)."""
        if not fitz or not Image:
            raise ImportError("PyMuPDF (fitz) v√† Pillow (PIL) c·∫ßn ƒë∆∞·ª£c c√†i ƒë·∫∑t ƒë·ªÉ chuy·ªÉn PDF th√†nh ·∫£nh")
        
        try:
            doc = fitz.open(pdf_path)
            page = doc.load_page(page_num)
            
            # Render page th√†nh pixmap v·ªõi ƒë·ªô ph√¢n gi·∫£i t·ªëi ∆∞u (gi·∫£m zoom ƒë·ªÉ tƒÉng t·ªëc)
            matrix = fitz.Matrix(zoom, zoom)
            pix = page.get_pixmap(matrix=matrix)
            
            # Chuy·ªÉn pixmap th√†nh PIL Image
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            
            doc.close()
            return img
        except Exception as e:
            logging.error(f"L·ªói chuy·ªÉn PDF page {page_num} th√†nh ·∫£nh: {e}")
            raise
    
    def convert_pdf_pages_to_images(self, pdf_path: str, max_pages: int = 200, max_size: int = 1600) -> List:
        """
        Chuy·ªÉn ƒë·ªïi t·ª´ng trang PDF th√†nh ·∫£nh PIL (t·ªëi ∆∞u t·ªëc ƒë·ªô)
        
        Args:
            pdf_path: ƒê∆∞·ªùng d·∫´n file PDF
            max_pages: S·ªë trang t·ªëi ƒëa ƒë·ªÉ x·ª≠ l√Ω
            max_size: K√≠ch th∆∞·ªõc t·ªëi ƒëa c·ªßa ·∫£nh (width ho·∫∑c height) ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
            
        Returns:
            List c√°c PIL Image (ƒë√£ ƒë∆∞·ª£c resize n·∫øu c·∫ßn)
        """
        if not fitz or not Image:
            raise ImportError("PyMuPDF (fitz) v√† Pillow (PIL) c·∫ßn ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        
        images = []
        try:
            # M·ªü PDF b·∫±ng PyMuPDF (fitz)
            doc = fitz.open(pdf_path)
            total_pages = min(doc.page_count, max_pages)
            logging.info(f"üìÑ ƒêang chuy·ªÉn ƒë·ªïi {total_pages} trang PDF th√†nh ·∫£nh...")
            
            # Gi·∫£m zoom t·ª´ 2x xu·ªëng 1.5x ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô
            zoom = 1.5
            matrix = fitz.Matrix(zoom, zoom)
            
            for page_num in range(total_pages):
                page = doc.load_page(page_num)
                
                # Render page th√†nh pixmap v·ªõi ƒë·ªô ph√¢n gi·∫£i t·ªëi ∆∞u
                pix = page.get_pixmap(matrix=matrix)
                
                # Chuy·ªÉn pixmap th√†nh PIL Image
                img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                
                # Resize ·∫£nh n·∫øu qu√° l·ªõn ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô encode v√† l∆∞u tr·ªØ
                if img.width > max_size or img.height > max_size:
                    img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                
                images.append(img)
                
                # Progress logging m·ªói 5 trang
                if (page_num + 1) % 5 == 0 or (page_num + 1) == total_pages:
                    logging.info(f"  ‚è≥ ƒê√£ x·ª≠ l√Ω {page_num + 1}/{total_pages} trang...")
            
            doc.close()
            logging.info(f"‚úÖ ƒê√£ chuy·ªÉn ƒë·ªïi {len(images)} trang th√†nh ·∫£nh (k√≠ch th∆∞·ªõc t·ªëi ƒëa: {max_size}px)")
            return images
        except Exception as e:
            logging.error(f"‚ùå L·ªói chuy·ªÉn PDF th√†nh ·∫£nh: {e}")
            raise
    
    def image_to_bytes(self, image, format: str = 'JPEG', quality: int = 85) -> bytes:
        """
        Chuy·ªÉn PIL Image th√†nh bytes ƒë·ªÉ l∆∞u v√†o MongoDB (t·ªëi ∆∞u k√≠ch th∆∞·ªõc)
        
        Args:
            image: PIL Image
            format: Format ·∫£nh (JPEG ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng, PNG n·∫øu c·∫ßn ch·∫•t l∆∞·ª£ng cao)
            quality: Ch·∫•t l∆∞·ª£ng JPEG (1-100, m·∫∑c ƒë·ªãnh 85)
            
        Returns:
            bytes
        """
        if not Image:
            raise ImportError("Pillow (PIL) c·∫ßn ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        
        buffer = __import__('io').BytesIO()
        if format.upper() == 'JPEG':
            # Convert RGBA to RGB n·∫øu c·∫ßn (JPEG kh√¥ng h·ªó tr·ª£ alpha)
            if image.mode == 'RGBA':
                rgb_image = Image.new('RGB', image.size, (255, 255, 255))
                rgb_image.paste(image, mask=image.split()[3])  # Use alpha channel as mask
                image = rgb_image
            image.save(buffer, format=format, quality=quality, optimize=True)
        else:
            image.save(buffer, format=format, optimize=True)
        return buffer.getvalue()
    
    def bytes_to_image(self, data: bytes):
        """
        Chuy·ªÉn bytes th√†nh PIL Image
        
        Args:
            data: bytes data
            
        Returns:
            PIL Image
        """
        if not Image:
            raise ImportError("Pillow (PIL) c·∫ßn ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        
        buffer = __import__('io').BytesIO(data)
        return Image.open(buffer)
