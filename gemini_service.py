import google.generativeai as genai
from groq import Groq
from config import Config
from typing import List, Dict, Optional, Set
import logging
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from datetime import datetime
try:
    from vintern_client import VinternClient
except ImportError:
    VinternClient = None
try:
    from PIL import Image
    from pdf_processor import PDFProcessor
    from database import db
except ImportError:
    Image = None
    PDFProcessor = None

class GeminiService:
    def __init__(self):
        # Delay configuring Gemini until first use to speed up app start
        # v√† h·ªó tr·ª£ nhi·ªÅu API key ƒë·ªÉ fallback
        self._gemini_keys = getattr(Config, "GEMINI_API_KEYS", [Config.GEMINI_API_KEY])
        self._current_key_index = 0
        self.model = None
        self._gemini_configured = False

        # M√¥ h√¨nh embedding text (fallback khi AI method tr·∫£ v·ªÅ qu√° √≠t chunks)
        self._text_embedding_model: Optional[SentenceTransformer] = None

        # Groq client (fallback khi to√†n b·ªô Gemini l·ªói / h·∫øt quota)
        self._groq_client: Optional[Groq] = None
        
        # Kh·ªüi t·∫°o Vintern client (n·∫øu c√≥)
        self.vintern = None
        if VinternClient:
            try:
                self.vintern = VinternClient()
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o Vintern client: {e}")
        
        # Kh·ªüi t·∫°o PDF processor ƒë·ªÉ x·ª≠ l√Ω ·∫£nh
        self.processor = None
        if PDFProcessor:
            try:
                self.processor = PDFProcessor()
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o PDF processor: {e}")

    def _ensure_gemini(self):
        """
        C·∫•u h√¨nh Gemini v·ªõi c∆° ch·∫ø fallback nhi·ªÅu API key.
        Th·ª≠ l·∫ßn l∆∞·ª£t c√°c key trong Config.GEMINI_API_KEYS cho t·ªõi khi kh·ªüi t·∫°o th√†nh c√¥ng.
        """
        if self._gemini_configured and self.model is not None:
            return

        last_error = None
        for idx, key in enumerate(self._gemini_keys):
            if not key:
                continue
            try:
                genai.configure(api_key=key)
                self.model = genai.GenerativeModel('gemini-2.5-flash-lite')
                self._current_key_index = idx
                self._gemini_configured = True
                logging.info(f"‚úÖ Kh·ªüi t·∫°o Gemini th√†nh c√¥ng v·ªõi key index {idx}")
                return
            except Exception as e:
                last_error = e
                logging.warning(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o Gemini v·ªõi key index {idx}: {e}")

        logging.error(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o Gemini v·ªõi b·∫•t k·ª≥ key n√†o: {last_error}")
        self.model = None
        self._gemini_configured = False

    def _rotate_gemini_key_and_reinit(self) -> bool:
        """
        Khi g·∫∑p l·ªói quota / 429 / auth, xoay sang key ti·∫øp theo v√† kh·ªüi t·∫°o l·∫°i model.
        """
        if not self._gemini_keys:
            return False
        start_index = self._current_key_index
        n = len(self._gemini_keys)
        last_error = None

        for step in range(1, n + 1):
            idx = (start_index + step) % n
            key = self._gemini_keys[idx]
            if not key:
                continue
            try:
                genai.configure(api_key=key)
                self.model = genai.GenerativeModel('gemini-2.5-flash-lite')
                self._current_key_index = idx
                self._gemini_configured = True
                logging.info(f"üîÅ ƒê·ªïi sang Gemini key index {idx} th√†nh c√¥ng")
                return True
            except Exception as e:
                last_error = e
                logging.warning(f"‚ö†Ô∏è L·ªói khi ƒë·ªïi sang Gemini key index {idx}: {e}")

        logging.error(f"‚ùå Kh√¥ng th·ªÉ xoay sang b·∫•t k·ª≥ Gemini key n√†o kh√°c: {last_error}")
        self.model = None
        self._gemini_configured = False
        return False

    def _ensure_groq(self):
        """
        Kh·ªüi t·∫°o Groq client n·∫øu c√≥ key. D√πng SDK ch√≠nh th·ª©c, kh√¥ng c·∫ßn base_url.
        """
        if self._groq_client is not None:
            return
        api_key = getattr(Config, "GROQ_API_KEY", None)
        if not api_key:
            logging.warning("GROQ_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh, b·ªè qua fallback Groq")
            return
        try:
            self._groq_client = Groq(api_key=api_key)
            logging.info("‚úÖ Kh·ªüi t·∫°o Groq client th√†nh c√¥ng")
        except Exception as e:
            logging.error(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o Groq client: {e}")
            self._groq_client = None

    def _ensure_text_embedding_model(self):
        """
        Kh·ªüi t·∫°o SentenceTransformer cho text embedding (fallback retrieval).
        """
        if self._text_embedding_model is not None:
            return
        try:
            self._text_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            logging.info("‚úÖ Kh·ªüi t·∫°o text embedding model (SentenceTransformer) th√†nh c√¥ng")
        except Exception as e:
            logging.error(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o text embedding model: {e}")
            self._text_embedding_model = None
    
    def generate_answer(self, question: str, context_chunks: List[Dict]) -> Dict:
        """T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ context chunks - h·ªó tr·ª£ c·∫£ text v√† image chunks"""
        try:
            # T√°ch chunks th√†nh text v√† image
            text_chunks = [c for c in context_chunks if c.get('chunk_type') != 'image']
            image_chunks = [c for c in context_chunks if c.get('chunk_type') == 'image']
            
            # N·∫øu c√≥ image chunks, d√πng Gemini Vision
            if image_chunks:
                return self._generate_answer_with_images(question, text_chunks, image_chunks)
            
            # N·∫øu ch·ªâ c√≥ text chunks, d√πng ph∆∞∆°ng th·ª©c c≈©
            return self._generate_answer_text_only(question, text_chunks)
            
        except Exception as e:
            logging.error(f"L·ªói t·∫°o c√¢u tr·∫£ l·ªùi: {e}")
            return {
                'answer': "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi.",
                'sources': [],
                'success': False,
                'error': str(e)
            }
    
    def _generate_answer_text_only(self, question: str, context_chunks: List[Dict]) -> Dict:
        """T·∫°o c√¢u tr·∫£ l·ªùi ch·ªâ t·ª´ text chunks"""
        try:
            # L·∫•y th·ªùi gian upload c·ªßa c√°c file ƒë·ªÉ x√°c ƒë·ªãnh file m·ªõi nh·∫•t
            file_upload_times = {}
            if db:
                unique_filenames = list(set(chunk.get('filename') for chunk in context_chunks if chunk.get('filename')))
                for filename in unique_filenames:
                    pdf_files = db.find_documents('pdf_files', {'filename': filename}, limit=1)
                    if pdf_files:
                        file_upload_times[filename] = pdf_files[0].get('created_at')
            
            # X√°c ƒë·ªãnh file m·ªõi nh·∫•t (upload g·∫ßn nh·∫•t)
            latest_file = None
            if file_upload_times:
                latest_file = max(file_upload_times.items(), key=lambda x: x[1] if x[1] else datetime.min)[0]
            
            # S·∫Øp x·∫øp chunks: file m·ªõi nh·∫•t l√™n tr∆∞·ªõc
            chunks_with_info = []
            for i, chunk in enumerate(context_chunks):
                chunk_filename = chunk.get('filename', 'N/A')
                is_latest = (latest_file and chunk_filename == latest_file)
                chunks_with_info.append((i, chunk, is_latest))
            
            # S·∫Øp x·∫øp: file m·ªõi nh·∫•t tr∆∞·ªõc, sau ƒë√≥ m·ªõi ƒë·∫øn file c≈©
            chunks_with_info.sort(key=lambda x: (not x[2], x[0]))
            
            # Build context text v√† sources v·ªõi th·ª© t·ª± m·ªõi
            context_text_sorted = ""
            sources = []
            for new_idx, (old_idx, chunk, is_latest) in enumerate(chunks_with_info):
                chunk_text = chunk.get('text', '') or chunk.get('content', '')
                chunk_filename = chunk.get('filename', 'N/A')
                
                file_info = chunk_filename
                if is_latest:
                    file_info += " [FILE M·ªöI NH·∫§T - Upload g·∫ßn nh·∫•t - B·∫¢N C·∫¨P NH·∫¨T]"
                elif chunk_filename in file_upload_times:
                    upload_time = file_upload_times[chunk_filename]
                    if upload_time:
                        if isinstance(upload_time, datetime):
                            time_str = upload_time.strftime("%d/%m/%Y %H:%M")
                        else:
                            time_str = str(upload_time)
                        file_info += f" [Upload: {time_str} - FILE C≈®]"
                
                context_text_sorted += f"Chunk {new_idx + 1} (File: {file_info}, Trang: {chunk.get('page_number', 'N/A')}):\n"
                context_text_sorted += chunk_text + "\n\n"
                
                # Build sources theo th·ª© t·ª± m·ªõi
                sources.append({
                    'filename': chunk_filename,
                    'page_number': chunk.get('page_number', 0),
                    'chunk_index': chunk.get('chunk_index', 0),
                    'char_start': chunk.get('char_start', 0),
                    'char_end': chunk.get('char_end', 0),
                    'chunk_id': chunk.get('chunk_id', chunk.get('_id', ''))
                })
            
            # T·∫°o prompt sau khi ƒë√£ build xong context v√† sources
            prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ vi·ªác tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu PDF. 
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin trong c√°c ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p.

Th√¥ng tin t√†i li·ªáu (ƒë√£ s·∫Øp x·∫øp: file m·ªõi nh·∫•t ·ªü tr√™n):
{context_text_sorted}

C√¢u h·ªèi: {question}

H∆∞·ªõng d·∫´n:
1. Tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin trong t√†i li·ªáu
2. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu"
3. Tr√≠ch d·∫´n ch√≠nh x√°c t·ª´ t√†i li·ªáu khi c√≥ th·ªÉ
4. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát c√≥ d·∫•u, kh√¥ng in ƒë·∫≠m
5. QUY T·∫ÆC QUAN TR·ªåNG NH·∫§T - ∆Øu ti√™n file m·ªõi nh·∫•t: 
   - N·∫øu c√≥ nhi·ªÅu file ch·ª©a th√¥ng tin v·ªÅ C√ôNG M·ªòT CH·ª¶ ƒê·ªÄ (v√≠ d·ª•: c√πng m·ªôt s·ª± ki·ªán, c√πng m·ªôt quy ƒë·ªãnh, c√πng m·ªôt l·ªãch tr√¨nh), B·∫ÆT BU·ªòC ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ file ƒë∆∞·ª£c ƒë√°nh d·∫•u [FILE M·ªöI NH·∫§T - Upload g·∫ßn nh·∫•t - B·∫¢N C·∫¨P NH·∫¨T].
   - KH√îNG ƒê∆Ø·ª¢C k·∫øt h·ª£p ho·∫∑c ƒë·ªÅ c·∫≠p ƒë·∫øn th√¥ng tin t·ª´ file c≈© n·∫øu file m·ªõi ƒë√£ c√≥ th√¥ng tin ƒë√≥.
   - Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ file c≈© (ƒë√°nh d·∫•u [FILE C≈®]) khi file m·ªõi KH√îNG ch·ª©a th√¥ng tin ƒë√≥.
   - File m·ªõi nh·∫•t l√† b·∫£n c·∫≠p nh·∫≠t, n√™n th√¥ng tin trong ƒë√≥ lu√¥n ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß h∆°n file c≈©.
6. R·∫§T QUAN TR·ªåNG: Sau ph·∫ßn tr·∫£ l·ªùi, h√£y th√™m m·ªôt d√≤ng duy nh·∫•t ch·ª©a M·ªòT ƒë·ªëi t∆∞·ª£ng JSON h·ª£p l·ªá v·ªõi hai kh√≥a: "text" v√† "images". Gi√° tr·ªã c·ªßa m·ªói kh√≥a l√† m·∫£ng c√°c ch·ªâ s·ªë (1-based) c·ªßa c√°c chunks b·∫°n th·ª±c s·ª± ƒë√£ S·ª¨ D·ª§NG ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi. V√≠ d·ª•:
    {{"text": [1, 3], "images": []}}
    N·∫øu b·∫°n kh√¥ng s·ª≠ d·ª•ng chunk n√†o, tr·∫£ v·ªÅ {{"text": [], "images": []}}.
7. Tuy·ªát ƒë·ªëi kh√¥ng in th√™m b·∫•t k·ª≥ danh s√°ch ngu·ªìn n√†o kh√°c ngo√†i d√≤ng JSON ƒë√≥ (kh√¥ng th√™m ch·ªØ "CHUNKS_USED" hay gi·∫£i th√≠ch).

Tr·∫£ l·ªùi:
"""
            
            # G·ªçi API Gemini (ensure configured) v·ªõi fallback nhi·ªÅu key + Groq
            response = None
            # 1) Th·ª≠ Gemini v·ªõi key hi·ªán t·∫°i, n·∫øu l·ªói quota/auth th√¨ rotate key
            for attempt in range(len(self._gemini_keys)):
                if not self.model:
                    self._ensure_gemini()
                if not self.model:
                    break
                try:
                    response = self.model.generate_content(prompt)
                    break
                except Exception as e:
                    msg = str(e).lower()
                    logging.warning(f"‚ö†Ô∏è L·ªói khi g·ªçi Gemini (attempt {attempt}): {e}")
                    # M·ªôt s·ªë l·ªói quota/429 ho·∫∑c auth ‚Üí th·ª≠ ƒë·ªïi key
                    if any(x in msg for x in ["429", "quota", "rate limit", "permission", "unauthorized", "invalid api key"]):
                        rotated = self._rotate_gemini_key_and_reinit()
                        if not rotated:
                            logging.error("‚ùå H·∫øt key Gemini kh·∫£ d·ª•ng, s·∫Ω fallback sang Groq")
                            self.model = None
                            break
                        continue
                    else:
                        # L·ªói kh√°c kh√¥ng ch·∫Øc do key ‚Üí tho√°t ƒë·ªÉ fallback Groq
                        self.model = None
                        break

            # 2) N·∫øu kh√¥ng c√≥ response t·ª´ Gemini ‚Üí fallback Groq n·∫øu c√≥
            if response is None:
                self._ensure_groq()
                if self._groq_client is None:
                    raise Exception("Kh√¥ng c√≥ Gemini c≈©ng nh∆∞ Groq kh·∫£ d·ª•ng")
                try:
                    groq_resp = self._groq_client.chat.completions.create(
                        model="llama-3.3-70b-versatile",
                        messages=[
                            {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω AI ti·∫øng Vi·ªát, tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c, d·ª±a tr√™n context PDF ƒë∆∞·ª£c cung c·∫•p."},
                            {"role": "user", "content": prompt},
                        ],
                        temperature=0.2,
                    )
                    answer = groq_resp.choices[0].message["content"]
                    # Kh√¥ng c√≥ CHUNKS_USED chu·∫©n t·ª´ Groq, n√™n tr·∫£ v·ªÅ to√†n b·ªô sources
                    return {
                        'answer': answer,
                        'sources': sources,
                        'success': True
                    }
                except Exception as ge:
                    logging.error(f"‚ùå L·ªói khi fallback Groq: {ge}")
                    raise Exception(f"Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi b·∫±ng Gemini/Groq: {ge}")

            try:
                answer = response.text
            except AttributeError:
                # Fallback n·∫øu response kh√¥ng c√≥ .text
                try:
                    answer = str(response.candidates[0].content.parts[0].text)
                except (AttributeError, IndexError, KeyError):
                    raise Exception("Kh√¥ng th·ªÉ l·∫•y text t·ª´ response Gemini")
            
            # T√°ch answer v√† chunks ƒë∆∞·ª£c d√πng b·∫±ng c√°ch t√¨m JSON footer {"text": [...], "images": [...]}
            used_chunks_indices = []
            actual_sources = []
            try:
                # Try to extract a JSON object at the end of the response
                jstart = answer.rfind('{')
                jend = answer.rfind('}')
                json_obj = None
                if jstart != -1 and jend != -1 and jend > jstart:
                    json_str = answer[jstart:jend+1]
                    import json as _json
                    try:
                        parsed = _json.loads(json_str)
                        # Remove JSON footer from answer
                        answer = answer[:jstart].strip()
                        text_indices = parsed.get('text', []) if isinstance(parsed.get('text', []), list) else []
                        # Convert to 0-based indices (sau khi s·∫Øp x·∫øp, sources ƒë√£ ƒë∆∞·ª£c build theo th·ª© t·ª± m·ªõi)
                        used_chunks_indices = [int(n) - 1 for n in text_indices if isinstance(n, int) or (isinstance(n, str) and n.isdigit())]
                    except Exception:
                        used_chunks_indices = []
                else:
                    used_chunks_indices = []
            except Exception as e:
                logging.warning(f"Kh√¥ng th·ªÉ parse JSON footer t·ª´ Gemini response: {e}")
                used_chunks_indices = []

            # Ch·ªâ l·∫•y sources c·ªßa chunks ƒë∆∞·ª£c s·ª≠ d·ª•ng; n·∫øu model kh√¥ng tr·∫£ v·ªÅ JSON ho·∫∑c m·∫£ng r·ªóng => tr·∫£ v·ªÅ sources r·ªóng ƒë·ªÉ tr√°nh g√°n nh·∫ßm ngu·ªìn
            if used_chunks_indices:
                actual_sources = [sources[i] for i in used_chunks_indices if 0 <= i < len(sources)]
            else:
                actual_sources = []
            
            return {
                'answer': answer,
                'sources': actual_sources,
                'success': True
            }
            
        except Exception as e:
            logging.error(f"L·ªói t·∫°o c√¢u tr·∫£ l·ªùi text only: {e}")
            return {
                'answer': "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi.",
                'sources': [],
                'success': False,
                'error': str(e)
            }
    
    def _generate_answer_with_images(self, question: str, text_chunks: List[Dict], 
                                    image_chunks: List[Dict]) -> Dict:
        """T·∫°o c√¢u tr·∫£ l·ªùi v·ªõi Gemini Vision ƒë·ªçc ·∫£nh"""
        try:
            if not self.model:
                self._ensure_gemini()
            if not self.model:
                raise Exception('Gemini kh√¥ng kh·∫£ d·ª•ng')
            
            # Load ·∫£nh t·ª´ database
            images = []
            image_sources = []
            
            for chunk in image_chunks:
                chunk_id = chunk.get('chunk_id') or chunk.get('_id')
                if not chunk_id:
                    continue
                
                # L·∫•y image_data t·ª´ database
                image_data = db.get_binary_field('pdf_chunks', {'chunk_id': chunk_id}, 'image_data')
                if image_data is not None and self.processor:
                    try:
                        # get_binary_field ƒë√£ tr·∫£ v·ªÅ bytes r·ªìi, kh√¥ng c·∫ßn convert l·∫°i
                        img = self.processor.bytes_to_image(image_data)
                        # Resize ·∫£nh n·∫øu qu√° l·ªõn (Gemini c√≥ gi·ªõi h·∫°n)
                        max_size = 2048
                        if img.width > max_size or img.height > max_size:
                            img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                        images.append(img)
                        # Th√¥ng tin file s·∫Ω ƒë∆∞·ª£c th√™m sau khi c√≥ file_upload_times
                        image_sources.append({
                            'filename': chunk.get('filename', 'N/A'),
                            'page_number': chunk.get('page_number', 0),
                            'chunk_index': chunk.get('chunk_index', 0),
                            'chunk_id': chunk_id,
                            'chunk_type': 'image'
                        })
                    except Exception as e:
                        filename = chunk.get('filename', 'unknown')
                        page_num = chunk.get('page_number', chunk.get('chunk_index', 0) + 1 if chunk.get('chunk_index') is not None else '?')
                        logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load ·∫£nh t·ª´ {filename} (trang {page_num}): {e}")
                else:
                    # Log khi kh√¥ng c√≥ image_data
                    filename = chunk.get('filename', 'unknown')
                    page_num = chunk.get('page_number', chunk.get('chunk_index', 0) + 1 if chunk.get('chunk_index') is not None else '?')
                    logging.debug(f"‚ö†Ô∏è Chunk {filename} (trang {page_num}) kh√¥ng c√≥ image_data")
            
            # L·∫•y th·ªùi gian upload c·ªßa c√°c file ƒë·ªÉ x√°c ƒë·ªãnh file m·ªõi nh·∫•t
            file_upload_times = {}
            if db:
                all_filenames = list(set(chunk.get('filename') for chunk in (text_chunks + image_chunks) if chunk.get('filename')))
                for filename in all_filenames:
                    pdf_files = db.find_documents('pdf_files', {'filename': filename}, limit=1)
                    if pdf_files:
                        file_upload_times[filename] = pdf_files[0].get('created_at')
            
            # X√°c ƒë·ªãnh file m·ªõi nh·∫•t
            latest_file = None
            if file_upload_times:
                latest_file = max(file_upload_times.items(), key=lambda x: x[1] if x[1] else datetime.min)[0]
            
            # Chu·∫©n b·ªã text context
            context_text = ""
            text_sources = []
            for i, chunk in enumerate(text_chunks):
                chunk_text = chunk.get('text', '') or chunk.get('content', '')
                chunk_filename = chunk.get('filename', 'N/A')
                
                # ƒê√°nh d·∫•u file m·ªõi nh·∫•t
                file_info = chunk_filename
                if latest_file and chunk_filename == latest_file:
                    file_info += " [FILE M·ªöI NH·∫§T - Upload g·∫ßn nh·∫•t]"
                elif chunk_filename in file_upload_times:
                    upload_time = file_upload_times[chunk_filename]
                    if upload_time:
                        if isinstance(upload_time, datetime):
                            time_str = upload_time.strftime("%d/%m/%Y %H:%M")
                        else:
                            time_str = str(upload_time)
                        file_info += f" [Upload: {time_str}]"
                
                context_text += f"Chunk {i+1} (File: {file_info}, Trang: {chunk.get('page_number', 'N/A')}):\n"
                context_text += chunk_text + "\n\n"
                
                text_sources.append({
                    'filename': chunk_filename,
                    'page_number': chunk.get('page_number', 0),
                    'chunk_index': chunk.get('chunk_index', 0),
                    'char_start': chunk.get('char_start', 0),
                    'char_end': chunk.get('char_end', 0),
                    'chunk_id': chunk.get('chunk_id', chunk.get('_id', '')),
                    'chunk_type': 'text'
                })
            
            # T·∫°o prompt v·ªõi ·∫£nh
            prompt_parts = []
            
            if context_text:
                prompt_parts.append(f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ vi·ªác tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu PDF. 
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin trong c√°c ƒëo·∫°n vƒÉn b·∫£n v√† h√¨nh ·∫£nh ƒë∆∞·ª£c cung c·∫•p.

Th√¥ng tin vƒÉn b·∫£n:
{context_text}

C√¢u h·ªèi: {question}

H∆∞·ªõng d·∫´n:
1. Tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin trong t√†i li·ªáu v√† h√¨nh ·∫£nh
2. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu"
3. Tr√≠ch d·∫´n ch√≠nh x√°c t·ª´ t√†i li·ªáu khi c√≥ th·ªÉ
4. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát c√≥ d·∫•u, kh√¥ng in ƒë·∫≠m
5. R·∫§T QUAN TR·ªåNG - ∆Øu ti√™n file m·ªõi nh·∫•t: N·∫øu c√≥ nhi·ªÅu file ch·ª©a th√¥ng tin t∆∞∆°ng t·ª± ho·∫∑c tr√πng l·∫∑p v·ªÅ c√πng m·ªôt ch·ªß ƒë·ªÅ, H√ÉY ∆ØU TI√äN S·ª¨ D·ª§NG TH√îNG TIN T·ª™ FILE ƒê∆Ø·ª¢C UPLOAD G·∫¶N NH·∫§T (file m·ªõi nh·∫•t). File m·ªõi nh·∫•t th∆∞·ªùng l√† b·∫£n c·∫≠p nh·∫≠t c·ªßa file c≈©, n√™n th√¥ng tin trong file m·ªõi nh·∫•t ch√≠nh x√°c v√† ƒë√°ng tin c·∫≠y h∆°n. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ file c≈© n·∫øu file m·ªõi kh√¥ng ch·ª©a th√¥ng tin ƒë√≥.
6. Cu·ªëi c√¢u tr·∫£ l·ªùi, h√£y th√™m d√≤ng "CHUNKS_USED:" v√† li·ªát k√™:
   - S·ªë th·ª© t·ª± c√°c text chunks b·∫°n ƒë√£ s·ª≠ d·ª•ng (v√≠ d·ª•: TEXT: 1, 3)
   - S·ªë th·ª© t·ª± c√°c h√¨nh ·∫£nh b·∫°n ƒë√£ s·ª≠ d·ª•ng (v√≠ d·ª•: IMAGES: 1, 2)
   (V√≠ d·ª•: CHUNKS_USED: TEXT: 1, 3 | IMAGES: 1, 2)

Tr·∫£ l·ªùi:
""")
            else:
                prompt_parts.append(f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ vi·ªác tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n h√¨nh ·∫£nh t·ª´ t√†i li·ªáu PDF.
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin trong c√°c h√¨nh ·∫£nh ƒë∆∞·ª£c cung c·∫•p.

C√¢u h·ªèi: {question}

H∆∞·ªõng d·∫´n:
1. Tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c d·ª±a tr√™n th√¥ng tin trong h√¨nh ·∫£nh
2. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu"
3. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát c√≥ d·∫•u, kh√¥ng in ƒë·∫≠m
4. R·∫§T QUAN TR·ªåNG - ∆Øu ti√™n file m·ªõi nh·∫•t: N·∫øu c√≥ nhi·ªÅu file ch·ª©a th√¥ng tin t∆∞∆°ng t·ª± ho·∫∑c tr√πng l·∫∑p v·ªÅ c√πng m·ªôt ch·ªß ƒë·ªÅ, H√ÉY ∆ØU TI√äN S·ª¨ D·ª§NG TH√îNG TIN T·ª™ FILE ƒê∆Ø·ª¢C UPLOAD G·∫¶N NH·∫§T (file m·ªõi nh·∫•t). File m·ªõi nh·∫•t th∆∞·ªùng l√† b·∫£n c·∫≠p nh·∫≠t c·ªßa file c≈©, n√™n th√¥ng tin trong file m·ªõi nh·∫•t ch√≠nh x√°c v√† ƒë√°ng tin c·∫≠y h∆°n.
5. Cu·ªëi c√¢u tr·∫£ l·ªùi, h√£y th√™m d√≤ng "CHUNKS_USED:" v√† li·ªát k√™ s·ªë th·ª© t·ª± c√°c h√¨nh ·∫£nh b·∫°n ƒë√£ s·ª≠ d·ª•ng (v√≠ d·ª•: CHUNKS_USED: IMAGES: 1, 2, 4)

Tr·∫£ l·ªùi:
""")
            
            # Th√™m ·∫£nh v√†o prompt
            for img in images:
                prompt_parts.append(img)
            
            # G·ªçi Gemini Vision (t·∫°m th·ªùi ch∆∞a fallback Groq v√¨ c·∫ßn h·ªó tr·ª£ h√¨nh ·∫£nh)
            response = self.model.generate_content(prompt_parts)
            try:
                answer = response.text
            except AttributeError:
                try:
                    answer = str(response.candidates[0].content.parts[0].text)
                except (AttributeError, IndexError, KeyError):
                    raise Exception("Kh√¥ng th·ªÉ l·∫•y text t·ª´ response Gemini")
            
            # T√°ch answer v√† chunks ƒë∆∞·ª£c d√πng t·ª´ format: CHUNKS_USED: TEXT: 1, 3 | IMAGES: 1, 2
            # Ho·∫∑c: CHUNKS_USED: IMAGES: 1, 2
            used_text_indices = []
            used_image_indices = []
            actual_sources = []
            
            try:
                # T√¨m d√≤ng CHUNKS_USED trong answer
                chunks_used_pattern = re.search(r'CHUNKS_USED:\s*(.+)', answer, re.IGNORECASE | re.MULTILINE)
                if chunks_used_pattern:
                    chunks_used_line = chunks_used_pattern.group(1).strip()
                    # Remove d√≤ng CHUNKS_USED kh·ªèi answer
                    answer = answer[:chunks_used_pattern.start()].strip()
                    
                    # Parse TEXT: 1, 3
                    text_match = re.search(r'TEXT:\s*([0-9,\s]+)', chunks_used_line, re.IGNORECASE)
                    if text_match:
                        text_nums = re.findall(r'\d+', text_match.group(1))
                        used_text_indices = [int(n) - 1 for n in text_nums if 0 <= int(n) - 1 < len(text_sources)]
                    
                    # Parse IMAGES: 1, 2
                    image_match = re.search(r'IMAGES?:\s*([0-9,\s]+)', chunks_used_line, re.IGNORECASE)
                    if image_match:
                        image_nums = re.findall(r'\d+', image_match.group(1))
                        used_image_indices = [int(n) - 1 for n in image_nums if 0 <= int(n) - 1 < len(image_sources)]
                    
                    logging.info(f"‚úÖ Parse CHUNKS_USED: TEXT={used_text_indices}, IMAGES={used_image_indices}")
                else:
                    # Fallback: th·ª≠ parse JSON format c≈©
                    jstart = answer.rfind('{')
                    jend = answer.rfind('}')
                    if jstart != -1 and jend != -1 and jend > jstart:
                        json_str = answer[jstart:jend+1]
                        import json as _json
                        try:
                            parsed = _json.loads(json_str)
                            answer = answer[:jstart].strip()
                            text_list = parsed.get('text', []) if isinstance(parsed.get('text', []), list) else []
                            image_list = parsed.get('images', []) if isinstance(parsed.get('images', []), list) else []
                            used_text_indices = [int(n) - 1 for n in text_list if isinstance(n, int) or (isinstance(n, str) and n.isdigit())]
                            used_image_indices = [int(n) - 1 for n in image_list if isinstance(n, int) or (isinstance(n, str) and n.isdigit())]
                            logging.info(f"‚úÖ Parse JSON footer: TEXT={used_text_indices}, IMAGES={used_image_indices}")
                        except Exception as e:
                            logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ parse JSON footer: {e}")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è L·ªói khi parse CHUNKS_USED: {e}")
            
            # N·∫øu kh√¥ng parse ƒë∆∞·ª£c indices nh∆∞ng c√≥ image chunks ƒë∆∞·ª£c truy·ªÅn v√†o,
            # th√¨ coi nh∆∞ t·∫•t c·∫£ image chunks ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng (fallback)
            if not used_image_indices and image_sources:
                logging.info(f"‚ö†Ô∏è Kh√¥ng parse ƒë∆∞·ª£c image indices, fallback: d√πng t·∫•t c·∫£ {len(image_sources)} image chunks")
                used_image_indices = list(range(len(image_sources)))
            
            # L·∫•y sources th·ª±c t·∫ø
            if used_text_indices:
                actual_sources.extend([text_sources[i] for i in used_text_indices if 0 <= i < len(text_sources)])
            if used_image_indices:
                actual_sources.extend([image_sources[i] for i in used_image_indices if 0 <= i < len(image_sources)])
            
            logging.info(f"üìå T·ªïng s·ªë sources: {len(actual_sources)} (text: {len([s for s in actual_sources if s.get('chunk_type') != 'image'])}, image: {len([s for s in actual_sources if s.get('chunk_type') == 'image'])})")
            
            return {
                'answer': answer,
                'sources': actual_sources,
                'success': True
            }
            
        except Exception as e:
            logging.error(f"L·ªói t·∫°o c√¢u tr·∫£ l·ªùi v·ªõi ·∫£nh: {e}")
            return {
                'answer': "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi.",
                'sources': [],
                'success': False,
                'error': str(e)
            }
    
    def find_relevant_chunks(self, question: str, all_chunks: List[Dict], top_k: int = 5) -> List[Dict]:
        """
        T√¨m chunks li√™n quan v·ªõi chi·∫øn l∆∞·ª£c hai t·∫ßng:
        1) D√πng AI method (Gemini) ƒë·ªÉ ch·ªçn c√°c ƒëo·∫°n quan tr·ªçng nh·∫•t (c·∫£ text v√† image).
        2) N·∫øu AI method tr·∫£ v·ªÅ √≠t h∆°n 3 chunks, b·ªï sung th√™m b·∫±ng:
           - Embedding-based retrieval cho text chunks
           - Vintern similarity cho image chunks
           ƒë·ªÉ ƒë·∫£m b·∫£o lu√¥n c√≥ t·ªëi ƒëa top_k (m·∫∑c ƒë·ªãnh 5) chunks ƒë∆∞a v√†o Gemini sinh c√¢u tr·∫£ l·ªùi.
        """
        try:
            if not all_chunks:
                return []

            # T√°ch text chunks v√† image chunks
            text_chunks = [c for c in all_chunks if c.get('chunk_type') != 'image']
            image_chunks = [c for c in all_chunks if c.get('chunk_type') == 'image']
            
            logging.info(f"üìä T·ªïng s·ªë chunks: {len(all_chunks)} (text: {len(text_chunks)}, image: {len(image_chunks)})")

            # B∆∞·ªõc 1: AI search (Gemini) tr√™n to√†n b·ªô chunks (c·∫£ text v√† image)
            logging.info("ü§ñ S·ª≠ d·ª•ng AI search (Gemini) ƒë·ªÉ t√¨m chunks li√™n quan")
            ai_chunks = self._find_relevant_chunks_ai(question, all_chunks, top_k)

            # N·∫øu AI ƒë√£ t√¨m ƒë∆∞·ª£c ƒë·ªß t·ªët (>=3 ho·∫∑c >=top_k) th√¨ d√πng lu√¥n
            if len(ai_chunks) >= min(3, top_k):
                logging.info(f"‚úÖ AI method t√¨m ƒë·ªß chunks: {len(ai_chunks)}")
                return ai_chunks[:top_k]

            # B∆∞·ªõc 2: Fallback b·∫±ng embedding ƒë·ªÉ b·ªï sung cho ƒë·ªß top_k
            logging.info(f"‚öôÔ∏è AI method ch·ªâ t√¨m ƒë∆∞·ª£c {len(ai_chunks)} chunks, fallback th√™m b·∫±ng embedding ƒë·ªÉ ƒë·ªß {top_k}")
            remaining = max(0, top_k - len(ai_chunks))
            if remaining == 0:
                return ai_chunks[:top_k]

            # ƒê√°nh d·∫•u c√°c chunks ƒë√£ ƒë∆∞·ª£c ch·ªçn
            selected_ids: Set[str] = set()
            for c in ai_chunks:
                cid = str(c.get('chunk_id') or c.get('_id') or f"{c.get('filename','')}#{c.get('chunk_index')}")
                selected_ids.add(cid)

            # B·ªï sung text chunks b·∫±ng embedding-based retrieval
            embed_text_chunks = self._find_relevant_chunks_embedding(question, text_chunks, remaining, selected_ids)
            
            # C·∫≠p nh·∫≠t selected_ids sau khi th√™m text chunks
            for c in embed_text_chunks:
                cid = str(c.get('chunk_id') or c.get('_id') or f"{c.get('filename','')}#{c.get('chunk_index')}")
                selected_ids.add(cid)
            
            # B·ªï sung image chunks b·∫±ng Vintern similarity (n·∫øu c√≤n slot v√† c√≥ image chunks)
            remaining_after_text = max(0, top_k - len(ai_chunks) - len(embed_text_chunks))
            embed_image_chunks = []
            if remaining_after_text > 0 and image_chunks and self.vintern:
                try:
                    embed_image_chunks = self._find_relevant_image_chunks_vintern(
                        question, image_chunks, remaining_after_text, selected_ids
                    )
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è L·ªói khi t√¨m image chunks b·∫±ng Vintern: {e}")

            # K·∫øt h·ª£p t·∫•t c·∫£ chunks
            combined = ai_chunks + embed_text_chunks + embed_image_chunks
            
            # Lo·∫°i tr√πng theo chunk_id
            seen: Set[str] = set()
            unique: List[Dict] = []
            for c in combined:
                cid = str(c.get('chunk_id') or c.get('_id') or f"{c.get('filename','')}#{c.get('chunk_index')}")
                if cid in seen:
                    continue
                seen.add(cid)
                unique.append(c)

            logging.info(f"üìå T·ªïng s·ªë chunks sau fallback: {len(unique)} (text: {sum(1 for c in unique if c.get('chunk_type') != 'image')}, image: {sum(1 for c in unique if c.get('chunk_type') == 'image')})")
            return unique[:top_k]
                
        except Exception as e:
            logging.error(f"L·ªói t√¨m ki·∫øm chunks: {e}")
            # Fallback cu·ªëi c√πng v·ªÅ chunks ƒë·∫ßu ti√™n
            return all_chunks[:top_k]
    
    def _find_relevant_chunks_ai(self, question: str, all_chunks: List[Dict], top_k: int = 5) -> List[Dict]:
        """AI method - t√¨m ki·∫øm semantic"""
        try:
            # L·∫•y th·ªùi gian upload c·ªßa c√°c file ƒë·ªÉ x√°c ƒë·ªãnh file m·ªõi nh·∫•t
            file_upload_times = {}
            if db:
                unique_filenames = list(set(chunk.get('filename') for chunk in all_chunks if chunk.get('filename')))
                for filename in unique_filenames:
                    pdf_files = db.find_documents('pdf_files', {'filename': filename}, limit=1)
                    if pdf_files:
                        file_upload_times[filename] = pdf_files[0].get('created_at')
            
            # X√°c ƒë·ªãnh file m·ªõi nh·∫•t
            latest_file = None
            if file_upload_times:
                latest_file = max(file_upload_times.items(), key=lambda x: x[1] if x[1] else datetime.min)[0]
            
            # T·∫°o prompt ƒë·ªÉ t√¨m ki·∫øm semantic
            search_prompt = f"""
            T√¨m ki·∫øm th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi: "{question}"

            C√°c ƒëo·∫°n vƒÉn b·∫£n v√† h√¨nh ·∫£nh:
            """
            
            for i, chunk in enumerate(all_chunks):
                chunk_text = chunk.get('text', '') or chunk.get('content', '')
                chunk_type = chunk.get('chunk_type', 'text')
                page_num = chunk.get('page_number', '?')
                filename = chunk.get('filename', 'unknown')
                
                # ƒê√°nh d·∫•u file m·ªõi nh·∫•t
                file_marker = ""
                if latest_file and filename == latest_file:
                    file_marker = " [FILE M·ªöI NH·∫§T - Upload g·∫ßn nh·∫•t]"
                elif filename in file_upload_times:
                    upload_time = file_upload_times[filename]
                    if upload_time:
                        if isinstance(upload_time, datetime):
                            time_str = upload_time.strftime("%d/%m/%Y")
                        else:
                            time_str = str(upload_time)
                        file_marker = f" [Upload: {time_str}]"
                
                if chunk_text:
                    # Text chunk: hi·ªÉn th·ªã n·ªôi dung
                    search_prompt += f"\nƒêo·∫°n {i+1} (VƒÉn b·∫£n, file {filename}{file_marker}, trang {page_num}): {chunk_text[:200]}..."
                elif chunk_type == 'image':
                    # Image chunk: m√¥ t·∫£ ng·∫Øn ƒë·ªÉ AI bi·∫øt c√≥ ·∫£nh
                    search_prompt += f"\nƒêo·∫°n {i+1} (H√¨nh ·∫£nh, file {filename}{file_marker}, trang {page_num}): [Trang {page_num} c·ªßa file {filename} - ch·ª©a h√¨nh ·∫£nh/scanned PDF, c√≥ th·ªÉ li√™n quan ƒë·∫øn c√¢u h·ªèi]"
            
            search_prompt += f"""
            \nH√£y x√°c ƒë·ªãnh c√°c ƒëo·∫°n (vƒÉn b·∫£n ho·∫∑c h√¨nh ·∫£nh) n√†o li√™n quan nh·∫•t ƒë·∫øn c√¢u h·ªèi "{question}".
            L∆ØU √ù: N·∫øu c√≥ nhi·ªÅu file ch·ª©a th√¥ng tin t∆∞∆°ng t·ª±, h√£y ∆∞u ti√™n ch·ªçn c√°c ƒëo·∫°n t·ª´ file ƒë∆∞·ª£c ƒë√°nh d·∫•u [FILE M·ªöI NH·∫§T - Upload g·∫ßn nh·∫•t] v√¨ ƒë√≥ l√† b·∫£n c·∫≠p nh·∫≠t m·ªõi nh·∫•t.
            Tr·∫£ l·ªùi ch·ªâ b·∫±ng s·ªë th·ª© t·ª± c√°c ƒëo·∫°n (v√≠ d·ª•: 1, 3, 5) ho·∫∑c "kh√¥ng c√≥" n·∫øu kh√¥ng t√¨m th·∫•y.
            """
            
            if not self.model:
                self._ensure_gemini()
            if not self.model:
                raise Exception('Gemini kh√¥ng kh·∫£ d·ª•ng')
            response = self.model.generate_content(search_prompt)
            relevant_indices = []
            
            # Parse response ƒë·ªÉ l·∫•y c√°c index
            try:
                response_text = response.text.strip()
            except AttributeError:
                # Fallback n·∫øu response kh√¥ng c√≥ .text
                try:
                    response_text = str(response.candidates[0].content.parts[0].text).strip()
                except (AttributeError, IndexError, KeyError):
                    logging.error("Kh√¥ng th·ªÉ l·∫•y text t·ª´ response")
                    return []
            if "kh√¥ng c√≥" not in response_text.lower():
                try:
                    # T√¨m c√°c s·ªë trong response
                    import re
                    numbers = re.findall(r'\d+', response_text)
                    relevant_indices = [int(num) - 1 for num in numbers if int(num) - 1 < len(all_chunks)]
                except:
                    pass
            
            # Tr·∫£ v·ªÅ top_k chunks li√™n quan nh·∫•t
            relevant_chunks = [all_chunks[i] for i in relevant_indices if i < len(all_chunks)]
            logging.info(f"AI method t√¨m th·∫•y {len(relevant_chunks)} chunks")

            # Log chi ti·∫øt c√°c chunk ƒë∆∞·ª£c ch·ªçn (kh√¥ng c√≥ score v√¨ l√† AI ch·ªçn theo th·ª© t·ª±)
            for rank, chunk in enumerate(relevant_chunks[:top_k], start=1):
                filename = chunk.get('filename', 'unknown')
                page_num = chunk.get('page_number', chunk.get('chunk_index', 0) + 1 if chunk.get('chunk_index') is not None else '?')
                chunk_idx = chunk.get('chunk_index', '?')
                logging.info(f"üîé Text chunk ƒë∆∞·ª£c ch·ªçn #{rank}: {filename} (chunk_index={chunk_idx}, trang {page_num})")
            
            return relevant_chunks[:top_k]
            
        except Exception as e:
            logging.error(f"L·ªói AI method: {e}")
            return []

    def _find_relevant_chunks_embedding(
        self,
        question: str,
        all_chunks: List[Dict],
        top_k: int,
        exclude_ids: Set[str]
    ) -> List[Dict]:
        """
        Fallback retrieval d·ª±a tr√™n embedding text khi AI method tr·∫£ v·ªÅ qu√° √≠t chunks.
        Ch·ªâ √°p d·ª•ng cho text chunks c√≥ s·∫µn tr∆∞·ªùng 'embedding' (list float).
        """
        try:
            if top_k <= 0:
                return []

            # Kh·ªüi t·∫°o embedding model
            self._ensure_text_embedding_model()
            if self._text_embedding_model is None:
                logging.warning("‚ö†Ô∏è Kh√¥ng c√≥ text embedding model, b·ªè qua fallback embedding")
                return []

            # L·ªçc c√°c text chunks c√≥ embedding v√† ch∆∞a b·ªã lo·∫°i tr·ª´
            candidates = []
            embeddings = []
            for chunk in all_chunks:
                if chunk.get('chunk_type') == 'image':
                    continue
                emb = chunk.get('embedding')
                if not emb:
                    continue
                cid = str(chunk.get('chunk_id') or chunk.get('_id') or f"{chunk.get('filename','')}#{chunk.get('chunk_index')}")
                if cid in exclude_ids:
                    continue
                try:
                    vec = np.array(emb, dtype=np.float32)
                except Exception:
                    continue
                candidates.append(chunk)
                embeddings.append(vec)

            if not candidates:
                logging.info("‚ö†Ô∏è Kh√¥ng c√≥ text chunk n√†o c√≥ embedding ƒë·ªÉ fallback")
                return []

            # T√≠nh embedding cho c√¢u h·ªèi
            q_vec = self._text_embedding_model.encode([question], convert_to_numpy=True)[0].astype(np.float32)

            # Chu·∫©n ho√° vector
            q_norm = np.linalg.norm(q_vec) + 1e-10
            q_vec = q_vec / q_norm
            E = np.stack(embeddings, axis=0)
            norms = np.linalg.norm(E, axis=1, keepdims=True) + 1e-10
            E_norm = E / norms

            scores = E_norm @ q_vec  # cosine similarity

            # L·∫•y top_k theo ƒëi·ªÉm similarity
            top_k = min(top_k, len(candidates))
            top_indices = np.argsort(-scores)[:top_k]

            results: List[Dict] = []
            for rank, idx in enumerate(top_indices, start=1):
                chunk = candidates[int(idx)]
                filename = chunk.get('filename', 'unknown')
                page_num = chunk.get('page_number', chunk.get('chunk_index', 0) + 1 if chunk.get('chunk_index') is not None else '?')
                chunk_idx = chunk.get('chunk_index', '?')
                logging.info(f"üìê Embedding fallback ch·ªçn chunk #{rank}: {filename} (chunk_index={chunk_idx}, trang {page_num}) - score: {scores[int(idx)]:.4f}")
                results.append(chunk)

            return results

        except Exception as e:
            logging.error(f"‚ùå L·ªói embedding fallback retrieval: {e}")
            return []
    
    def _find_relevant_image_chunks_vintern(
        self,
        question: str,
        image_chunks: List[Dict],
        top_k: int,
        exclude_ids: Set[str]
    ) -> List[Dict]:
        """
        T√¨m image chunks li√™n quan b·∫±ng Vintern similarity.
        """
        try:
            if top_k <= 0 or not image_chunks:
                return []
            
            if not self.vintern or not self.vintern.is_available():
                logging.warning("‚ö†Ô∏è Vintern kh√¥ng kh·∫£ d·ª•ng, b·ªè qua t√¨m image chunks")
                return []
            
            # L·ªçc c√°c image chunks c√≥ embedding v√† ch∆∞a b·ªã lo·∫°i tr·ª´
            candidates = []
            embeddings = []
            for chunk in image_chunks:
                cid = str(chunk.get('chunk_id') or chunk.get('_id') or f"{chunk.get('filename','')}#{chunk.get('chunk_index')}")
                if cid in exclude_ids:
                    continue
                
                embedding_data = chunk.get('embedding_data')
                if not embedding_data:
                    continue
                
                try:
                    # Convert bytes to tensor
                    import torch
                    emb_tensor = self.vintern.bytes_to_embedding(embedding_data)
                    candidates.append(chunk)
                    embeddings.append(emb_tensor)
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load embedding cho image chunk {cid}: {e}")
                    continue
            
            if not candidates:
                logging.info("‚ö†Ô∏è Kh√¥ng c√≥ image chunk n√†o c√≥ embedding ƒë·ªÉ t√¨m ki·∫øm")
                return []
            
            # Encode c√¢u h·ªèi th√†nh embedding
            query_embedding = self.vintern.encode_query(question)
            
            # T√≠nh similarity
            scores = self.vintern.compute_similarity(query_embedding, embeddings)
            
            # L·∫•y top_k theo ƒëi·ªÉm similarity
            top_k = min(top_k, len(candidates))
            top_indices = torch.argsort(scores, descending=True)[:top_k]
            
            results: List[Dict] = []
            for rank, idx in enumerate(top_indices, start=1):
                chunk = candidates[int(idx)]
                filename = chunk.get('filename', 'unknown')
                page_num = chunk.get('page_number', '?')
                chunk_idx = chunk.get('chunk_index', '?')
                score = float(scores[int(idx)])
                logging.info(f"üñºÔ∏è Vintern ch·ªçn image chunk #{rank}: {filename} (chunk_index={chunk_idx}, trang {page_num}) - score: {score:.4f}")
                results.append(chunk)
            
            return results
            
        except Exception as e:
            logging.error(f"‚ùå L·ªói t√¨m image chunks b·∫±ng Vintern: {e}")
            return []
    
    def _find_relevant_chunks_hybrid(self, question: str, text_chunks: List[Dict], 
                                    image_chunks: List[Dict], top_k: int = 5) -> List[Dict]:
        """T√¨m ki·∫øm hybrid s·ª≠ d·ª•ng Vintern cho c·∫£ text v√† image"""
        try:
            relevant_chunks = []
            
            # 1. T√¨m text chunks li√™n quan b·∫±ng AI method
            if text_chunks:
                text_relevant = self._find_relevant_chunks_ai(question, text_chunks, top_k=top_k//2 + 1)
                relevant_chunks.extend(text_relevant)
            
            # 2. T√¨m image chunks li√™n quan b·∫±ng Vintern similarity
            if image_chunks and self.vintern:
                try:
                    # Encode query
                    query_embedding = self.vintern.encode_query(question)
                    
                    # Load embeddings t·ª´ database
                    doc_embeddings = []
                    valid_image_chunks = []
                    
                    for chunk in image_chunks:
                        chunk_id = chunk.get('chunk_id') or chunk.get('_id')
                        if not chunk_id:
                            continue
                        
                        # L·∫•y embedding_data t·ª´ database
                        embedding_data = db.get_binary_field('pdf_chunks', {'chunk_id': chunk_id}, 'embedding_data')
                        if embedding_data is not None:
                            try:
                                # get_binary_field ƒë√£ tr·∫£ v·ªÅ bytes r·ªìi, kh√¥ng c·∫ßn convert l·∫°i
                                embedding = self.vintern.bytes_to_embedding(embedding_data)
                                doc_embeddings.append(embedding)
                                valid_image_chunks.append(chunk)
                            except Exception as e:
                                filename = chunk.get('filename', 'unknown')
                                page_num = chunk.get('page_number', chunk.get('chunk_index', 0) + 1 if chunk.get('chunk_index') is not None else '?')
                                logging.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load embedding t·ª´ {filename} (trang {page_num}): {e}")
                        else:
                            # Log khi kh√¥ng c√≥ embedding_data
                            filename = chunk.get('filename', 'unknown')
                            page_num = chunk.get('page_number', chunk.get('chunk_index', 0) + 1 if chunk.get('chunk_index') is not None else '?')
                            logging.debug(f"‚ö†Ô∏è Chunk {filename} (trang {page_num}) kh√¥ng c√≥ embedding_data")
                    
                    if doc_embeddings:
                        # T√≠nh similarity
                        scores = self.vintern.compute_similarity(query_embedding, doc_embeddings)
                        
                        # L·∫•y top image chunks
                        if len(scores.shape) > 0:
                            scores_list = scores.cpu().numpy().tolist()
                            if isinstance(scores_list[0], list):
                                scores_list = scores_list[0]
                            
                            # S·∫Øp x·∫øp theo score
                            scored_chunks = list(zip(valid_image_chunks, scores_list))
                            scored_chunks.sort(key=lambda x: x[1], reverse=True)

                            # Log top-n image chunks theo score ƒë·ªÉ th·∫•y to√†n b·ªô qu√° tr√¨nh
                            max_log = min(5, len(scored_chunks))
                            logging.info(f"üìä Top {max_log} image chunks theo similarity cho c√¢u h·ªèi: \"{question}\"")
                            for rank, (chunk, score) in enumerate(scored_chunks[:max_log], start=1):
                                filename = chunk.get('filename', 'unknown')
                                page_num = chunk.get('page_number', chunk.get('chunk_index', 0) + 1 if chunk.get('chunk_index') is not None else '?')
                                chunk_idx = chunk.get('chunk_index', '?')
                                logging.info(f"  #{rank}: {filename} (chunk_index={chunk_idx}, trang {page_num}) - score: {score:.4f}")
                            
                            # L·∫•y top image chunks th·ª±c s·ª± d√πng cho tr·∫£ l·ªùi
                            top_image_count = min(top_k - len(relevant_chunks), len(scored_chunks))
                            for chunk, score in scored_chunks[:top_image_count]:
                                relevant_chunks.append(chunk)
                                # V·∫´n gi·ªØ log ng·∫Øn g·ªçn cho c√°c chunk cu·ªëi c√πng ƒë∆∞·ª£c ch·ªçn
                                filename = chunk.get('filename', 'unknown')
                                page_num = chunk.get('page_number', chunk.get('chunk_index', 0) + 1 if chunk.get('chunk_index') is not None else '?')
                                logging.info(f"üì∏ Image chunk ƒë∆∞·ª£c ch·ªçn: {filename} (trang {page_num}) - score: {score:.4f}")
                except Exception as e:
                    logging.warning(f"‚ö†Ô∏è L·ªói t√¨m ki·∫øm image chunks: {e}")
                    # Fallback: th√™m m·ªôt v√†i image chunks ƒë·∫ßu ti√™n
                    relevant_chunks.extend(image_chunks[:top_k - len(relevant_chunks)])
            
            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng chunks
            return relevant_chunks[:top_k]
            
        except Exception as e:
            logging.error(f"L·ªói hybrid search: {e}")
            # Fallback: k·∫øt h·ª£p text v√† image chunks
            combined = text_chunks[:top_k//2] + image_chunks[:top_k//2]
            return combined[:top_k]